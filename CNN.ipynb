{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CNN Creation and development",
   "id": "7a2b8b0ba0d232bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T16:13:55.242654Z",
     "start_time": "2024-11-24T16:13:55.089451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Audio processing libraries\n",
    "import librosa\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For saving and loading data\n",
    "import pickle\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "id": "f78dc2a554385d5a",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[0;32m/usr/lib/python3/dist-packages/numpy/core/__init__.py:24\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 24\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m multiarray\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[0;32m/usr/lib/python3/dist-packages/numpy/core/multiarray.py:10\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mfunctools\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m overrides\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _multiarray_umath\n",
      "File \u001B[0;32m/usr/lib/python3/dist-packages/numpy/core/overrides.py:8\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_utils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_inspect\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m getargspec\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_multiarray_umath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      9\u001B[0m     add_docstring,  _get_implementing_args, _ArrayFunctionDispatcher)\n\u001B[1;32m     12\u001B[0m ARRAY_FUNCTIONS \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'numpy.core._multiarray_umath'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[0;32m/usr/lib/python3/dist-packages/numpy/__init__.py:130\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 130\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__config__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m show \u001B[38;5;28;01mas\u001B[39;00m show_config\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/usr/lib/python3/dist-packages/numpy/__config__.py:4\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01menum\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Enum\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_multiarray_umath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      5\u001B[0m     __cpu_features__,\n\u001B[1;32m      6\u001B[0m     __cpu_baseline__,\n\u001B[1;32m      7\u001B[0m     __cpu_dispatch__,\n\u001B[1;32m      8\u001B[0m )\n\u001B[1;32m     10\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshow\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m/usr/lib/python3/dist-packages/numpy/core/__init__.py:50\u001B[0m\n\u001B[1;32m     27\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     28\u001B[0m \n\u001B[1;32m     29\u001B[0m \u001B[38;5;124mIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m \u001B[38;5;241m%\u001B[39m (sys\u001B[38;5;241m.\u001B[39mversion_info[\u001B[38;5;241m0\u001B[39m], sys\u001B[38;5;241m.\u001B[39mversion_info[\u001B[38;5;241m1\u001B[39m], sys\u001B[38;5;241m.\u001B[39mexecutable,\n\u001B[1;32m     49\u001B[0m         __version__, exc)\n\u001B[0;32m---> 50\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(msg)\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[0;31mImportError\u001B[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.11 from \"/usr/bin/python3.11\"\n  * The NumPy version is: \"1.26.4\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: No module named 'numpy.core._multiarray_umath'\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlibrosa\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msoundfile\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msf\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3/dist-packages/numpy/__init__.py:135\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    132\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mError importing numpy: you should not try to import numpy from\u001B[39m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;124m    its source directory; please exit the numpy source tree, and relaunch\u001B[39m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;124m    your python interpreter from there.\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m--> 135\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    137\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexceptions\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mModuleDeprecationWarning\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVisibleDeprecationWarning\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mComplexWarning\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTooHardError\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAxisError\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    141\u001B[0m \u001B[38;5;66;03m# mapping of {name: (value, deprecation_msg)}\u001B[39;00m\n",
      "\u001B[0;31mImportError\u001B[0m: Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there."
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "definir constantes e paths",
   "id": "ef344731ced4b3e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define constants\n",
    "TARGET_SAMPLE_RATE = 22050  # Sampling rate for audio files\n",
    "TARGET_LENGTH = 4  # Target length in seconds\n",
    "N_FFT = 2048  # Number of samples in FFT\n",
    "HOP_LENGTH = 512  # Number of samples between successive frames\n",
    "\n",
    "# Paths to dataset and output files\n",
    "FOLDS_PATH = 'UrbanSound8K/audio'\n",
    "DATA_INFO_PATH = 'UrbanSound8K/metadata/UrbanSound8K.csv' \n",
    "OUTPUT_PKL = 'urbansound8k.pkl'\n"
   ],
   "id": "d9a233d0f42fe26d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T16:11:50.544920Z",
     "start_time": "2024-11-24T16:11:50.505482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load metadata\n",
    "data_info = pd.read_csv(DATA_INFO_PATH)\n",
    "data_info.head()"
   ],
   "id": "19dfe9f15b52bb55",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./UrbanSound8K/metadata/UrbanSound8K.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m df\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Normalizar o audio para mater o standart de 4 segundos",
   "id": "b45a6decb60e8277"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_and_pad_audio(file_path, target_sr, target_length):\n",
    "    \"\"\"\n",
    "    Loads an audio file, resamples it to the target sample rate,\n",
    "    and pads or trims it to the target length.\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(file_path, sr=target_sr)\n",
    "    target_samples = int(target_length * sr)\n",
    "    \n",
    "    if len(y) > target_samples:\n",
    "        # Trim the audio to the target length\n",
    "        y = y[:target_samples]\n",
    "    else:\n",
    "        # Pad the audio with zeros (silence) to reach the target length\n",
    "        padding = target_samples - len(y)\n",
    "        y = np.pad(y, (0, padding), 'constant')\n",
    "    \n",
    "    return y\n"
   ],
   "id": "8f6981c971d9d951"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data augmentation",
   "id": "65c771d5bd1ccbf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random\n",
    "\n",
    "def augment_audio(y, sr):\n",
    "    \"\"\"\n",
    "    Apply random augmentation techniques to an audio signal.\n",
    "    \n",
    "    Parameters:\n",
    "        y (np.ndarray): Audio time series.\n",
    "        sr (int): Sampling rate of y.\n",
    "        \n",
    "    Returns:\n",
    "        List[np.ndarray]: List of augmented audio signals.\n",
    "    \"\"\"\n",
    "    augmented_audios = []\n",
    "    \n",
    "    # Time Stretching\n",
    "    if random.choice([True, False]):\n",
    "        rate = random.uniform(0.8, 1.2)\n",
    "        y_stretch = librosa.effects.time_stretch(y, rate)\n",
    "        augmented_audios.append(y_stretch)\n",
    "    \n",
    "    # Pitch Shifting\n",
    "    if random.choice([True, False]):\n",
    "        n_steps = random.randint(-2, 2)\n",
    "        y_shift = librosa.effects.pitch_shift(y, sr, n_steps)\n",
    "        augmented_audios.append(y_shift)\n",
    "    \n",
    "    # Adding Noise\n",
    "    if random.choice([True, False]):\n",
    "        noise_amp = 0.005 * np.random.uniform() * np.amax(y)\n",
    "        y_noise = y + noise_amp * np.random.normal(size=y.shape[0])\n",
    "        augmented_audios.append(y_noise)\n",
    "    \n",
    "    return augmented_audios\n"
   ],
   "id": "9d4179d1a825c083"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Damos process aos audio files e extraimos features. Iteramos sobre cada ficheiro de audio, damos load e extraimos features para usar depois",
   "id": "7d7f70d93dab974a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_features(y, sr):\n",
    "    \"\"\"\n",
    "    Extracts multiple audio features from a given audio signal.\n",
    "    \n",
    "    Parameters:\n",
    "        y (np.ndarray): Audio time series.\n",
    "        sr (int): Sampling rate of y.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: 2D array of stacked features.\n",
    "    \"\"\"\n",
    "    # Mel Spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    \n",
    "    # MFCCs\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    \n",
    "    # Stack features vertically (along the feature axis)\n",
    "    features = np.vstack((mel_spec_db, mfcc))\n",
    "    \n",
    "    return features\n"
   ],
   "id": "213cb6493361eb45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize data structures\n",
    "feature_list = []\n",
    "label_list = []\n",
    "fold_list = []\n",
    "\n",
    "if not os.path.exists(OUTPUT_PKL):\n",
    "    folds = [fold for fold in os.listdir(FOLDS_PATH) if \"fold\" in fold]\n",
    "    for fold in folds:\n",
    "        print(f\"Processing {fold}...\")\n",
    "        audio_files = librosa.util.find_files(os.path.join(FOLDS_PATH, fold))\n",
    "        for i, audio in enumerate(audio_files):\n",
    "            file_name = os.path.basename(audio)\n",
    "            # Retrieve class ID from metadata\n",
    "            matching_rows = data_info.loc[data_info['slice_file_name'] == file_name, 'classID']\n",
    "            if not matching_rows.empty:\n",
    "                classid = matching_rows.values[0]\n",
    "            else:\n",
    "                print(f\"No matching classID found for {file_name}\")\n",
    "                continue  # Skip this file\n",
    "            \n",
    "            # Load and pad audio\n",
    "            y = load_and_pad_audio(audio, TARGET_SAMPLE_RATE, TARGET_LENGTH)\n",
    "            \n",
    "            # Extract features from original audio\n",
    "            features = extract_features(y, TARGET_SAMPLE_RATE)\n",
    "            \n",
    "            # Append original features\n",
    "            feature_list.append(features)\n",
    "            label_list.append(classid)\n",
    "            fold_list.append(fold)\n",
    "            \n",
    "            # Data Augmentation\n",
    "            augmented_audios = augment_audio(y, TARGET_SAMPLE_RATE)\n",
    "            for aug_y in augmented_audios:\n",
    "                aug_features = extract_features(aug_y, TARGET_SAMPLE_RATE)\n",
    "                # Append augmented features\n",
    "                feature_list.append(aug_features)\n",
    "                label_list.append(classid)\n",
    "                fold_list.append(fold)\n",
    "            \n",
    "            # Progress logging\n",
    "            if i % 100 == 0:\n",
    "                print(f\"{i} files processed in {fold}\")\n",
    "        print(f\"Finished processing {fold}\")\n",
    "    print(\"Feature extraction and augmentation completed.\")\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(feature_list)\n",
    "    y = np.array(label_list)\n",
    "    folds_array = np.array(fold_list)\n",
    "    \n",
    "    # Save the data\n",
    "    with open(OUTPUT_PKL, 'wb') as f:\n",
    "        pickle.dump((X, y, folds_array), f)\n",
    "    print(\"Processed data saved successfully.\")\n",
    "else:\n",
    "    # Load processed data from the pickle file\n",
    "    with open(OUTPUT_PKL, 'rb') as f:\n",
    "        X, y, folds_array = pickle.load(f)\n",
    "    print(\"Processed data loaded successfully.\")\n"
   ],
   "id": "7622b1ae7743e12d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Agora normalizamos as features, convertemos lists para arrays e aplicamos o min-max scaling",
   "id": "4698edbdf91c1bc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if not os.path.exists(OUTPUT_PKL):\n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(feature_list)\n",
    "    y = np.array(label_list)\n",
    "    folds_array = np.array(fold_list)\n",
    "    # Normalize features to [0, 1]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n"
   ],
   "id": "b17a55175827589d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preparação da data para Model training",
   "id": "93ec7a016eb2f84f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check the shape of features\n",
    "print(f\"Feature shape before reshaping: {X.shape}\")  # Expected: (num_samples, num_features_rows, num_features_columns)\n",
    "\n",
    "# Ensure all features have the same shape\n",
    "feature_shapes = [feat.shape for feat in X]\n",
    "unique_shapes = set(feature_shapes)\n",
    "print(f\"Unique feature shapes: {unique_shapes}\")\n",
    "\n",
    "# If all features have the same shape, stack them into a 4D array\n",
    "X_array = np.stack(X, axis=0)\n",
    "print(f\"Features shape after stacking: {X_array.shape}\")  # (num_samples, height, width)\n",
    "\n",
    "# Expand dimensions for CNN input\n",
    "X_array = np.expand_dims(X_array, axis=-1)  # Shape: (num_samples, height, width, channels)\n",
    "print(f\"Features shape after expanding dimensions: {X_array.shape}\")\n"
   ],
   "id": "88a62984edc72446"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Aplicamos one-hot encoding para as class ficarem suitable para treino",
   "id": "a8f550482795c697"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# One-Hot Encode labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "print(f'Encoded labels shape: {y_encoded.shape}')\n"
   ],
   "id": "2093825dbc7c1e86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split da data em treino, validation e testing sets",
   "id": "5ad4b0b502a33a2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use Stratified K-Fold Cross-Validation\n",
    "num_folds = 5\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store results\n",
    "fold_metrics = []\n",
    "fold_no = 1\n",
    "\n",
    "for train_index, test_index in skf.split(X_array, y):\n",
    "    print(f'\\nTraining fold {fold_no}...')\n",
    "    X_train, X_test = X_array[train_index], X_array[test_index]\n",
    "    y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "    \n",
    "    # Further split training data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "    \n",
    "    print(f'Training samples: {X_train.shape[0]}, Validation samples: {X_val.shape[0]}, Testing samples: {X_test.shape[0]}')\n",
    "    \n",
    "    # Proceed to model definition and training\n",
    "    # ...\n",
    "    \n",
    "    # Break after one fold for demonstration\n",
    "    fold_no += 1\n",
    "    break\n"
   ],
   "id": "20efb4915cf92ab3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "CNN Model",
   "id": "9b7e7812bf554d39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_cnn_model(input_shape, num_classes=10):\n",
    "    \"\"\"\n",
    "    Creates a Convolutional Neural Network model.\n",
    "    \n",
    "    Parameters:\n",
    "        input_shape (tuple): Shape of the input data (height, width, channels).\n",
    "        num_classes (int): Number of output classes.\n",
    "        \n",
    "    Returns:\n",
    "        keras.Model: Compiled CNN model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Convolutional Block\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Third Convolutional Block\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Flatten and Dense Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_shape = X_train.shape[1:]  # (height, width, channels)\n",
    "model = create_cnn_model(input_shape)\n",
    "model.summary()\n"
   ],
   "id": "965e1031365ca103"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compilação do Modelo",
   "id": "aa49eb32700ba319"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ],
   "id": "c0094aeda723c678"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "DEfinir callbacks",
   "id": "f7669a46a6c141c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n"
   ],
   "id": "ba1a065f2bcb12a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Treinar o modelo",
   "id": "64280bae2328a330"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[early_stopping, reduce_lr],\n",
    "                    verbose=1)\n"
   ],
   "id": "116c7b882d9249b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Avaliar o modelo",
   "id": "c94b5059ba6e2e9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')\n"
   ],
   "id": "380e11693a59c415"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Graficos",
   "id": "412501782c1ffb51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "766ad8597074b680"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Classification report e confusion matrix",
   "id": "e05f7ddcceb3be9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "class_labels = encoder.categories_[0]\n",
    "print('Classification Report')\n",
    "print(classification_report(y_true, y_pred_classes, target_names=class_labels.astype(str)))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ],
   "id": "45840a3405b2b077"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

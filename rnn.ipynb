{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, we will be adressing our take on the problem using a **recurrent neural network**.\n",
    "\n",
    "We will begin  by importing the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm # progress bar on long runs\n",
    "from scipy.io import wavfile as wav\n",
    "import librosa\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras import regularizers as reg\n",
    "from keras import layers\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per mentioned in the **project statement**, the target variable corresponds to the correct labeling of the sound. There are 10 different possible sounds in the dataset:\n",
    "\n",
    " - air conditioner\n",
    " - car horn\n",
    " - children playing\n",
    " - dog bark\n",
    " - drilling\n",
    " - engine idling\n",
    " - gun shot\n",
    " - jackhammer\n",
    " - siren\n",
    " - street music\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already find the `classID` column, which essentially represents each label as an integer, from 0 to 9:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classID: 0, class: air_conditioner\n",
      "classID: 1, class: car_horn\n",
      "classID: 2, class: children_playing\n",
      "classID: 3, class: dog_bark\n",
      "classID: 4, class: drilling\n",
      "classID: 5, class: engine_idling\n",
      "classID: 6, class: gun_shot\n",
      "classID: 7, class: jackhammer\n",
      "classID: 8, class: siren\n",
      "classID: 9, class: street_music\n"
     ]
    }
   ],
   "source": [
    "class_id_pairs = df[['classID', 'class']].drop_duplicates().sort_values(by=\"classID\")\n",
    "\n",
    "for index, row in class_id_pairs.iterrows():\n",
    "    print(f'classID: {row[\"classID\"]}, class: {row[\"class\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we can remove the last column and begin working with our dataset, which we already determined is slightly unbalanced for the `car_horn` and `gunshot` values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3\n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2\n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2\n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2\n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['class'],inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since not every `.wav` file is 4 seconds long, we will apply **zero-padding** to ensure that all files meet this requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /////////////////// NEEDS REVISION ///////////////////\n",
    "\n",
    "# I don't understand how to do this in the np arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve dataset consistency and traning data quality, we also decided to create 2 new datasets: \n",
    "\n",
    " - `df_22`: resamples data to 22050Hz\n",
    " - `df_44`: resamples data to 44100Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEED HELP\n",
    "\n",
    "not sure if i should do resampling before feature extraction or during feature extraction. during feature extraction would probably help automate different quality sound extraction. but it will depend on the 0-padding order aswell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librosa extracts MFCCs on different scales for different .wav files. This is due to the fact that lower frequencies are emphasized during this process, potentially creating bias issues as a consequence of heterogeneous distributions of frequencies throughout each file.\n",
    "\n",
    "To address this issue, we can apply feature scaling to the new dataframes, in order to improve data quality for our modeling purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Iterates over all original dataframe rows (predicts approximate runtime)\\nfor index_num,row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\", unit=\"row\"):\\n    # Get the \"features\" array for the current row\\n    features_array = row[\\'feature\\']\\n\\n    # Ensure the features_array is a 2D array (in case it is 1D)\\n    # If it\\'s a 1D array of shape (40,) for example, reshape it into (40, 1) for scaling\\n    if isinstance(features_array, np.ndarray):  # Check if the element is a numpy array\\n        if features_array.ndim == 1:\\n            # Reshape the 1D array to 2D for scaling\\n            features_array = features_array.reshape(-1, 1)\\n        \\n        # Apply Min-Max scaling to the array\\n        scaled_features = scaler.fit_transform(features_array).flatten()  # Flatten to maintain 1D structure after scaling\\n        \\n        # Update the \"features\" column with the scaled array (in-place)\\n        df.at[index_num, \\'feature\\'] = scaled_features\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses sklearn's MinMax scaler, rescales values to be in a range of [0,1]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# /////////////////// NEEDS REVISION ///////////////////\n",
    "\n",
    "# example = df_2d.iloc[0][\"feature\"][0]\n",
    "# print(\"First arrray of the first entry in the 2D dataset: \\n\", example)\n",
    "\n",
    "'''\n",
    "# Iterates over all original dataframe rows (predicts approximate runtime)\n",
    "for index_num,row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\", unit=\"row\"):\n",
    "    # Get the \"features\" array for the current row\n",
    "    features_array = row['feature']\n",
    "\n",
    "    # Ensure the features_array is a 2D array (in case it is 1D)\n",
    "    # If it's a 1D array of shape (40,) for example, reshape it into (40, 1) for scaling\n",
    "    if isinstance(features_array, np.ndarray):  # Check if the element is a numpy array\n",
    "        if features_array.ndim == 1:\n",
    "            # Reshape the 1D array to 2D for scaling\n",
    "            features_array = features_array.reshape(-1, 1)\n",
    "        \n",
    "        # Apply Min-Max scaling to the array\n",
    "        scaled_features = scaler.fit_transform(features_array).flatten()  # Flatten to maintain 1D structure after scaling\n",
    "        \n",
    "        # Update the \"features\" column with the scaled array (in-place)\n",
    "        df.at[index_num, 'feature'] = scaled_features\n",
    "'''\n",
    "# /////////////////// NEEDS REVISION ///////////////////"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "The **librosa** library has a built-in method for feature extraction, called [Mel-Frequency Cepstral Coefficients](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum), that summarises the frequency distribution across the time window.\n",
    "\n",
    "In order to build the new dataset, we developed the following functions, which are capable of extracting **1D or 2D** features.\n",
    "\n",
    "These feature extractor functions will represent the frequencies found in the wav files as **np arrays**, while using MFCCs in order to obtain features similar to the way humans perceive sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the mean from the Time axis, uses file sample rate\n",
    "def features_extractor_1D(file):\n",
    "    audio, sample_rate = librosa.load(file) \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0) \n",
    "    return mfccs_scaled_features\n",
    "\n",
    "# Uses both Time and Frequency axis, custom sample rate\n",
    "def features_extractor_2D(file, sample_rate, path=True):\n",
    "    if path: audio, _ = librosa.load(file) \n",
    "    else: audio = file\n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    return mfccs_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to transform audio files into usable data types, we must associate each numpy array to their respective entry inside the df dataframe.\n",
    "\n",
    "This will allow for important pre-processing steps to be applied accordingly, as well as proper Neural Network training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to identify files with duration < 4s and apply zero padding for consistency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(file_path, target_sr=44100, target_length=4):\n",
    "    \"\"\"\n",
    "    Loads an audio file, resamples it to the target sample rate,\n",
    "    and pads or trims it to the target length.\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(file_path, sr=target_sr)\n",
    "    target_samples = int(target_length * sr)\n",
    "    \n",
    "    if len(y) > target_samples:\n",
    "        # Trim the audio to the target length\n",
    "        y = y[:target_samples]\n",
    "    else:\n",
    "        # Pad the audio with zeros (silence) to reach the target length\n",
    "        padding = target_samples - len(y)\n",
    "        y = np.pad(y, (0, padding), 'constant')\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Iterates over all original dataframe rows (predicts approximate runtime)\\nfor index_num,row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\", unit=\"row\"):\\n    # Identifies wav file name, concatenates to respective fold: accesses original .wav file\\n    #file_name = os.path.join(os.path.abspath(audio_dataset_path),\\'fold\\'+str(row[\"fold\"])+\\'/\\',str(row[\"slice_file_name\"]))\\n    file_name = os.path.join(os.path.abspath(audio_dataset_path),\\'fold\\'+str(row[\"fold\"])+\\'\\\\\\',str(row[\"slice_file_name\"]))\\n    \\n    # Adds associated sound label\\n    final_class_labels=row[\"classID\"]\\n\\n    y = zero_padding(file_name)\\n\\n    # 22050Hz sample rate\\n    data1=features_extractor_2D(y, 22050, False) \\n    extracted_features22.append([data1,final_class_labels])\\n\\n    # 44100hHz sample rate\\n    data2=features_extractor_2D(y, 44100, False) \\n    extracted_features44.append([data2,final_class_labels])\\n    \\n\\n# Convert extracted_features to Pandas dataframe\\ndf_1d =pd.DataFrame(extracted_features22,columns=[\\'feature\\',\\'class\\'])\\ndf_2d =pd.DataFrame(extracted_features44,columns=[\\'feature\\',\\'class\\'])\\n\\ndf_1d.to_csv(\"rnn_2d_22.csv\", index=False)\\ndf_2d.to_csv(\"rnn_2d_44.csv\", index=False)\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify path containing all folds\n",
    "audio_dataset_path='../UrbanSound8K/audio/'\n",
    "extracted_features22=[]\n",
    "extracted_features44=[]\n",
    "\n",
    "'''\n",
    "\n",
    "# Iterates over all original dataframe rows (predicts approximate runtime)\n",
    "for index_num,row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\", unit=\"row\"):\n",
    "    # Identifies wav file name, concatenates to respective fold: accesses original .wav file\n",
    "    #file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'\\\\',str(row[\"slice_file_name\"]))\n",
    "    \n",
    "    # Adds associated sound label\n",
    "    final_class_labels=row[\"classID\"]\n",
    "\n",
    "    y = zero_padding(file_name)\n",
    "\n",
    "    # 22050Hz sample rate\n",
    "    data1=features_extractor_2D(y, 22050, False) \n",
    "    extracted_features22.append([data1,final_class_labels])\n",
    "\n",
    "    # 44100hHz sample rate\n",
    "    data2=features_extractor_2D(y, 44100, False) \n",
    "    extracted_features44.append([data2,final_class_labels])\n",
    "    \n",
    "\n",
    "# Convert extracted_features to Pandas dataframe\n",
    "df_1d =pd.DataFrame(extracted_features22,columns=['feature','class'])\n",
    "df_2d =pd.DataFrame(extracted_features44,columns=['feature','class'])\n",
    "\n",
    "df_1d.to_csv(\"rnn_2d_22.csv\", index=False)\n",
    "df_2d.to_csv(\"rnn_2d_44.csv\", index=False)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding common pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original [UrbanSound8k](https://urbansounddataset.weebly.com/urbansound8k.html) website features a section outlined specifically for **cross validation**.\n",
    "\n",
    "These rules emphasize the specific creation of 10 folds, following the identifiers present in the `fold` column from the original csv. It is explained that different folds display different levels of classification difficulty, which could potentially invalidate model efficiency if not performed correctly. \n",
    "\n",
    "Since both the `df_44` and `df_22` dataframes were created using the row order of the df_dataframe, we know that the `fold` column will be correctly aligned with the rest of the data. This means that we can select specific rows and assing them to their respective fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                feature  class\n",
      "64    [[-5.1526788e+02 -5.1202197e+02 -5.1161533e+02...      3\n",
      "65    [[-5.1560699e+02 -5.1592670e+02 -5.1592078e+02...      3\n",
      "66    [[-140.91316    -180.47021    -253.62337    .....      3\n",
      "105   [[-3.3049469e+02 -2.8215903e+02 -2.9419110e+02...      3\n",
      "106   [[-2.4541306e+02 -1.5865527e+02 -1.4228606e+02...      6\n",
      "...                                                 ...    ...\n",
      "8676  [[-194.6469     -211.81342    -275.955      .....      9\n",
      "8677  [[-176.86174   -200.68466   -229.09007   ... -...      9\n",
      "8678  [[-222.71817    -225.29225    -237.86415    .....      9\n",
      "8679  [[-146.61781   -161.50307   -233.15102   ... -...      9\n",
      "8680  [[-150.59117    -168.43904    -232.6128     .....      9\n",
      "\n",
      "[873 rows x 2 columns]\n",
      "Data successfully split into 10 folds. Initiate model development...\n"
     ]
    }
   ],
   "source": [
    "# Avoids constant running of feature extraction\n",
    "df_44 = pd.read_csv('44100_padded.csv')\n",
    "\n",
    "# Create a dictionary to store the folds for df_44\n",
    "folds = {}\n",
    "\n",
    "# i iterates from 1 through 10\n",
    "for i in range(1, 11):\n",
    "    # row r with fold i in df\n",
    "    # corresponds to row n in df_44\n",
    "    # adds to specific fold in dictionary\n",
    "    folds[i] = df_44[df['fold'] == i]\n",
    "\n",
    "print(folds[1])\n",
    "\n",
    "print(\"Data successfully split into 10 folds. Initiate model development...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to develop an effective **Recurrent Neural Network**, the group decided to explore the concept of **Long Short Term Memory** (LSTM) networks. LSTMs are a type of RNN that are designed to handle sequential data pattern recognition. \n",
    "\n",
    "We consider this approach could be the most effective in order to classify the sounds, since continuous sounds or repetitive rythms are sequential. These time-dependant aspects are characteristics which LSTMs are capable of recognizing and \"remembering\" throughout training.\n",
    "\n",
    "In this case, each LSTM layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build this LSTM model, we decided to do some research and attempt to look into the topology suggested in a few published papers, before attempting to change and improve classification performance.\n",
    "\n",
    "The [first paper](https://dergi.neu.edu.tr/index.php/aiit/article/download/740/327/3147) suggests the following:\n",
    "\n",
    " - 22050Hz sample rate\n",
    " - 1 LSTM layer of size 128\n",
    " - followed another LSTM layer of size 64\n",
    " - SoftMax for prediction\n",
    " - 50 epochs\n",
    " - Adam optimization\n",
    " - Dropout Rate = 0.2\n",
    "\n",
    "This topology takes the large input vector size into consideration, meaning that it should maintain robustness and generally avoid major overfiting, while still allowing the model to identify patterns during an appropriate ammount of time. It also uses as sufficiently good sound quality for environment noise purposes, although we would like to verify if the difference in sound quality allows for better classification. A dropout rate of 0.2 (rate in which neurons are randomly disabled) is used in order to prevent overfitting.\n",
    "\n",
    "Overall, this model aims for efficiency and computation speed, in an effort to reduce training time while preserving model quality and performance, hence why we chose this as our starter model topology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[The second paper](https://annals-csis.org/Volume_18/drp/pdf/185.pdf) proposes almost identical topology (similar to most of the projects found), with the small changes of:\n",
    " - 44100Hz sample rate\n",
    " - 64 epochs\n",
    " - Dropout Rate = 0.25\n",
    "\n",
    "We expect this model to take longer to train (due to the higher number of epochs).\n",
    "It could lead to overfitting, which is why the **dropout rate** has also been **adjusted** to the slightly higher value of 0.25. If it is correctly trained, it **should demonstrate better results**, compared to the previous settings.\n",
    "\n",
    "Overall, it is a generally \"riskier\" model, and it was chosen to assess if the results of the combination combination of **increase in epochs** and **improved sound quality** could compensate the time consumed, render the training problems relativelly irrelevant if the generalization and classification abilities of the resulting model show significant improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We expect the **first model** to be trained faster due to inferior sound quality and lower epochs, in contrast with the **second model**, which could overfit but benefits from higher sound quality and dropout rate, potentially mitigating such problems, overall being preferable.\n",
    "\n",
    "Both results will be used as a type of foundation for the final topology decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REVIEW !!!\n",
    "\n",
    "If we have time, we can try:\n",
    "\n",
    " - 2 LSTM layers of size 128 (might take a while)\n",
    " - SoftMax for combination of 2D results\n",
    " - Sigmoid or Tanh for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the previously stated network topology and parameters, and using the available **tensorflow** and **keras** modules, we can start implementing an LSTM neural network, using available tools like:\n",
    "\n",
    " - `keras.Sequential()`: \n",
    " - `layers.LSTM`: \n",
    " - `layers.Dropout`:\n",
    " - `layers.TimeDistributed`: \n",
    " - `layers.Dropout`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a Neural Network generating function, which is based on the following concepts: \n",
    "\n",
    " - By ensuring that the input has a fixed shape and frequency, we can now use a Sequential keras model. This will allow for simple layer stacking and parameter configuration;\n",
    " - A dropout rate of 0.2 is employed in between each layer (with the exception of flatten and classification layers);\n",
    " - Time Distributed layers allow for easier handling of the extracted sequential features, by preserving the sequential time dimension features while reducing dimensionality;\n",
    " - In order to potentially improve our classifier's robustness, we decided to employ **l2 regularization** (or Lasso regularization), due to our dataset's reduced size;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easily configurable to generate with different architecture\n",
    "# Implicitly creates first model, explained above\n",
    "\n",
    "def generate_lstm(x, y, lstm1_size=128, lstm2_size=64, dropout=0.2):\n",
    "    \n",
    "    # Uses UrbanSounds8k split data shape\n",
    "    train_shape = (x, y) \n",
    "\n",
    "    # Initiaizes sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adds 2 LSTM layers of size 128 and 64, with a dropout rate of 0.2\n",
    "    model.add(layers.LSTM(lstm1_size,  input_shape = train_shape, return_sequences = True, activation='tanh'))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "\n",
    "    # The second layer does not need return sequences, \n",
    "    # since the next layer won't be another LSTM\n",
    "    model.add(layers.LSTM(lstm2_size, return_sequences = False, activation='tanh'))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "\n",
    "    # Suggested intermediate dense layer -> reduce dimensionality and preserve time sequential features\n",
    "    model.add(layers.TimeDistributed(layers.Dense(64, activation='tanh', kernel_regularizer = reg.l2(0.01))))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.TimeDistributed(layers.Dense(32, activation='tanh', kernel_regularizer = reg.l2(0.01))))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "\n",
    "    # /////////////////// NEEDS REVISION ///////////////////\n",
    "\n",
    "    # Last TimeDistributed produces 16 features/time step (thought dimensionality was appropriate)\n",
    "    model.add(layers.TimeDistributed(layers.Dense(16, activation='tanh', kernel_regularizer = reg.l2(0.01))))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "\n",
    "    # Flattens the last TimeDistributed outputs\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Dense output layer -> classification\n",
    "    model.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STILL NEEDS TO BE COMPILED AND ADAM OPTIMIZED!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on folds: (1, 2, 3, 4, 5, 6, 7, 8, 9), Testing on fold: 10\n",
      "X_train shape: (7895,), y_train shape: (7895,)\n",
      "X_test shape: (837,), y_test shape: (837,)\n",
      "Training on folds: (1, 2, 3, 4, 5, 6, 7, 8, 10), Testing on fold: 9\n",
      "X_train shape: (7916,), y_train shape: (7916,)\n",
      "X_test shape: (816,), y_test shape: (816,)\n",
      "Training on folds: (1, 2, 3, 4, 5, 6, 7, 9, 10), Testing on fold: 8\n",
      "X_train shape: (7926,), y_train shape: (7926,)\n",
      "X_test shape: (806,), y_test shape: (806,)\n",
      "Training on folds: (1, 2, 3, 4, 5, 6, 8, 9, 10), Testing on fold: 7\n",
      "X_train shape: (7894,), y_train shape: (7894,)\n",
      "X_test shape: (838,), y_test shape: (838,)\n",
      "Training on folds: (1, 2, 3, 4, 5, 7, 8, 9, 10), Testing on fold: 6\n",
      "X_train shape: (7909,), y_train shape: (7909,)\n",
      "X_test shape: (823,), y_test shape: (823,)\n",
      "Training on folds: (1, 2, 3, 4, 6, 7, 8, 9, 10), Testing on fold: 5\n",
      "X_train shape: (7796,), y_train shape: (7796,)\n",
      "X_test shape: (936,), y_test shape: (936,)\n",
      "Training on folds: (1, 2, 3, 5, 6, 7, 8, 9, 10), Testing on fold: 4\n",
      "X_train shape: (7742,), y_train shape: (7742,)\n",
      "X_test shape: (990,), y_test shape: (990,)\n",
      "Training on folds: (1, 2, 4, 5, 6, 7, 8, 9, 10), Testing on fold: 3\n",
      "X_train shape: (7807,), y_train shape: (7807,)\n",
      "X_test shape: (925,), y_test shape: (925,)\n",
      "Training on folds: (1, 3, 4, 5, 6, 7, 8, 9, 10), Testing on fold: 2\n",
      "X_train shape: (7844,), y_train shape: (7844,)\n",
      "X_test shape: (888,), y_test shape: (888,)\n",
      "Training on folds: (2, 3, 4, 5, 6, 7, 8, 9, 10), Testing on fold: 1\n",
      "X_train shape: (7859,), y_train shape: (7859,)\n",
      "X_test shape: (873,), y_test shape: (873,)\n"
     ]
    }
   ],
   "source": [
    "def test_index(indexes):\n",
    "    for i in range(1,11):\n",
    "        if i not in indexes: return i\n",
    "    print(\"Something went wrong in index for test.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# different fold combinatiions\n",
    "comb_list = list(combinations(folds, 9))\n",
    "\n",
    "for pair in comb_list:\n",
    "    # Concatenates all 9 training folds for this round\n",
    "    X_train = np.concatenate([folds[i]['feature'].values for i in pair], axis=0)\n",
    "    y_train = np.concatenate([folds[i]['class'].values for i in pair], axis=0)\n",
    "    \n",
    "    # Gets test fold for this round\n",
    "    test_fold = folds[test_index(pair)]  \n",
    "\n",
    "    # Splits\n",
    "    X_test = test_fold['feature'].values\n",
    "    y_test = test_fold['class'].values\n",
    "    \n",
    "    # Now, you can use X_train, y_train, X_test, and y_test for training and evaluation\n",
    "    print(f\"Training on folds: {pair}, Testing on fold: {test_index(pair)}\")\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid dtype: tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Generate the LSTM model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[37], line 13\u001b[0m, in \u001b[0;36mgenerate_lstm\u001b[1;34m(x, y, lstm1_size, lstm2_size, dropout)\u001b[0m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Adds 2 LSTM layers of size 128 and 64, with a dropout rate of 0.2\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm1_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_sequences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtanh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mDropout(dropout))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# The second layer does not need return sequences, \u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# since the next layer won't be another LSTM\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hasht\\anaconda3\\envs\\urban-class\\Lib\\site-packages\\keras\\src\\models\\sequential.py:88\u001b[0m, in \u001b[0;36mSequential.add\u001b[1;34m(self, layer, rebuild)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(layer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_input_shape_arg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(\u001b[43mInputLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_shape_arg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# If we are passed a Keras tensor created by keras.Input(), we\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# extract the input layer from its keras history and use that.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(layer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_history\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\hasht\\anaconda3\\envs\\urban-class\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:47\u001b[0m, in \u001b[0;36mInputLayer.__init__\u001b[1;34m(self, shape, batch_size, dtype, sparse, batch_shape, input_tensor, optional, name, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must pass a `shape` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     shape \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandardize_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m (batch_size,) \u001b[38;5;241m+\u001b[39m shape\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_shape \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mstandardize_shape(batch_shape)\n",
      "File \u001b[1;32mc:\\Users\\hasht\\anaconda3\\envs\\urban-class\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py:534\u001b[0m, in \u001b[0;36mstandardize_shape\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mbackend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_DimExpr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(e)):\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;66;03m# JAX2TF tracing uses JAX-native dimension expressions\u001b[39;00m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_int_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    536\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to a shape. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    537\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound invalid entry \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    538\u001b[0m     )\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hasht\\anaconda3\\envs\\urban-class\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py:565\u001b[0m, in \u001b[0;36mis_int_dtype\u001b[1;34m(dtype)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.backend.is_int_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_int_dtype\u001b[39m(dtype):\n\u001b[1;32m--> 565\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[43mstandardize_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hasht\\anaconda3\\envs\\urban-class\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py:506\u001b[0m, in \u001b[0;36mstandardize_dtype\u001b[1;34m(dtype)\u001b[0m\n\u001b[0;32m    503\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mALLOWED_DTYPES:\n\u001b[1;32m--> 506\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dtype\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid dtype: tuple"
     ]
    }
   ],
   "source": [
    "# Generate the LSTM model\n",
    "model = generate_lstm(X_train.shape[1:], y_train.shape[1])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Many-to-Many LSTM for Sequence Prediction with TimeDistributed layers - [link](https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, we will be adressing our take on the problem using a **recurrent neural network**.\n",
    "\n",
    "We will begin  by importing the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T15:55:17.352163Z",
     "start_time": "2024-11-24T15:55:16.588435Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm # progress bar on long runs\n",
    "from scipy.io import wavfile as wav\n",
    "import librosa\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras import regularizers as reg\n",
    "from keras import layers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm \u001B[38;5;66;03m# progress bar on long runs\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3/dist-packages/pandas/__init__.py:19\u001B[0m\n\u001B[1;32m     16\u001B[0m         _missing_dependencies\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_dependency\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_e\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _missing_dependencies:  \u001B[38;5;66;03m# pragma: no cover\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to import required dependencies:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(_missing_dependencies)\n\u001B[1;32m     21\u001B[0m     )\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;66;03m# numpy compat\u001B[39;00m\n",
      "\u001B[0;31mImportError\u001B[0m: Unable to import required dependencies:\nnumpy: Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there."
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per mentioned in the **project statement**, the target variable corresponds to the correct labeling of the sound. There are 10 different possible sounds in the dataset:\n",
    "\n",
    " - air conditioner\n",
    " - car horn\n",
    " - children playing\n",
    " - dog bark\n",
    " - drilling\n",
    " - engine idling\n",
    " - gun shot\n",
    " - jackhammer\n",
    " - siren\n",
    " - street music\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already find the `classID` column, which essentially represents each label as an integer, from 0 to 9:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classID: 0, class: air_conditioner\n",
      "classID: 1, class: car_horn\n",
      "classID: 2, class: children_playing\n",
      "classID: 3, class: dog_bark\n",
      "classID: 4, class: drilling\n",
      "classID: 5, class: engine_idling\n",
      "classID: 6, class: gun_shot\n",
      "classID: 7, class: jackhammer\n",
      "classID: 8, class: siren\n",
      "classID: 9, class: street_music\n"
     ]
    }
   ],
   "source": [
    "class_id_pairs = df[['classID', 'class']].drop_duplicates().sort_values(by=\"classID\")\n",
    "\n",
    "for index, row in class_id_pairs.iterrows():\n",
    "    print(f'classID: {row[\"classID\"]}, class: {row[\"class\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we can remove the last column and begin working with our dataset, which we already determined is slightly unbalanced for the `car_horn` and `gunshot` values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3\n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2\n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2\n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2\n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['class'],inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since not every `.wav` file is 4 seconds long, we will apply **zero-padding** to ensure that all files meet this requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /////////////////// NEEDS REVISION ///////////////////\n",
    "\n",
    "# I don't understand how to do this in the np arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve dataset consistency and traning data quality, we also decided to create 2 new datasets: \n",
    "\n",
    " - `df_22`: resamples data to 22050Hz\n",
    " - `df_44`: resamples data to 44100Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEED HELP\n",
    "\n",
    "not sure if i should do resampling before feature extraction or during feature extraction. during feature extraction would probably help automate different quality sound extraction. but it will depend on the 0-padding order aswell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librosa extracts MFCCs on different scales for different .wav files. This is due to the fact that lower frequencies are emphasized during this process, potentially creating bias issues as a consequence of heterogeneous distributions of frequencies throughout each file.\n",
    "\n",
    "To address this issue, we can apply feature scaling to the new dataframes, in order to improve data quality for our modeling purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Iterates over all original dataframe rows (predicts approximate runtime)\\nfor index_num,row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\", unit=\"row\"):\\n    # Get the \"features\" array for the current row\\n    features_array = row[\\'feature\\']\\n\\n    # Ensure the features_array is a 2D array (in case it is 1D)\\n    # If it\\'s a 1D array of shape (40,) for example, reshape it into (40, 1) for scaling\\n    if isinstance(features_array, np.ndarray):  # Check if the element is a numpy array\\n        if features_array.ndim == 1:\\n            # Reshape the 1D array to 2D for scaling\\n            features_array = features_array.reshape(-1, 1)\\n        \\n        # Apply Min-Max scaling to the array\\n        scaled_features = scaler.fit_transform(features_array).flatten()  # Flatten to maintain 1D structure after scaling\\n        \\n        # Update the \"features\" column with the scaled array (in-place)\\n        df.at[index_num, \\'feature\\'] = scaled_features\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses sklearn's MinMax scaler, rescales values to be in a range of [0,1]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# /////////////////// NEEDS REVISION ///////////////////\n",
    "\n",
    "# example = df_2d.iloc[0][\"feature\"][0]\n",
    "# print(\"First arrray of the first entry in the 2D dataset: \\n\", example)\n",
    "\n",
    "'''\n",
    "# Iterates over all original dataframe rows (predicts approximate runtime)\n",
    "for index_num,row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\", unit=\"row\"):\n",
    "    # Get the \"features\" array for the current row\n",
    "    features_array = row['feature']\n",
    "\n",
    "    # Ensure the features_array is a 2D array (in case it is 1D)\n",
    "    # If it's a 1D array of shape (40,) for example, reshape it into (40, 1) for scaling\n",
    "    if isinstance(features_array, np.ndarray):  # Check if the element is a numpy array\n",
    "        if features_array.ndim == 1:\n",
    "            # Reshape the 1D array to 2D for scaling\n",
    "            features_array = features_array.reshape(-1, 1)\n",
    "        \n",
    "        # Apply Min-Max scaling to the array\n",
    "        scaled_features = scaler.fit_transform(features_array).flatten()  # Flatten to maintain 1D structure after scaling\n",
    "        \n",
    "        # Update the \"features\" column with the scaled array (in-place)\n",
    "        df.at[index_num, 'feature'] = scaled_features\n",
    "'''\n",
    "# /////////////////// NEEDS REVISION ///////////////////"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "The **librosa** library has a built-in method for feature extraction, called [Mel-Frequency Cepstral Coefficients](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum), that summarises the frequency distribution across the time window.\n",
    "\n",
    "In order to build the new dataset, we developed the following functions, which are capable of extracting **1D or 2D** features.\n",
    "\n",
    "These feature extractor functions will represent the frequencies found in the wav files as **np arrays**, while using MFCCs in order to obtain features similar to the way humans perceive sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the mean from the Time axis, uses file sample rate\n",
    "def features_extractor_1D(file):\n",
    "    audio, sample_rate = librosa.load(file) \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0) \n",
    "    return mfccs_scaled_features\n",
    "\n",
    "# Uses both Time and Frequency axis, custom sample rate\n",
    "def features_extractor_2D(file, sample_rate):\n",
    "    audio, _ = librosa.load(file) \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    return mfccs_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to transform audio files into usable data types, we must associate each numpy array to their respective entry inside the df dataframe.\n",
    "\n",
    "This will allow for important pre-processing steps to be applied accordingly, as well as proper Neural Network training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 8732/8732 [03:25<00:00, 42.51row/s]\n"
     ]
    }
   ],
   "source": [
    "# Identify path containing all folds\n",
    "audio_dataset_path='../UrbanSound8K/audio/'\n",
    "extracted_features22=[]\n",
    "extracted_features44=[]\n",
    "\n",
    "'''\n",
    "\n",
    "# Iterates over all original dataframe rows (predicts approximate runtime)\n",
    "for index_num,row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\", unit=\"row\"):\n",
    "    # Identifies wav file name, concatenates to respective fold: accesses original .wav file\n",
    "    #file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'\\\\',str(row[\"slice_file_name\"]))\n",
    "    \n",
    "    # Adds associated sound label\n",
    "    final_class_labels=row[\"classID\"]\n",
    "    \n",
    "    # 22050Hz sample rate\n",
    "    data1=features_extractor_2D(file_name, 22050) \n",
    "    extracted_features22.append([data1,final_class_labels])\n",
    "\n",
    "    # 44100hHz sample rate\n",
    "    data2=features_extractor_2D(file_name, 44100) \n",
    "    extracted_features44.append([data2,final_class_labels])\n",
    "    \n",
    "\n",
    "# Convert extracted_features to Pandas dataframe\n",
    "df_1d =pd.DataFrame(extracted_features22,columns=['feature','class'])\n",
    "df_2d =pd.DataFrame(extracted_features44,columns=['feature','class'])\n",
    "\n",
    "df_1d.to_csv(\"rnn_2d_22.csv\", index=False)\n",
    "df_2d.to_csv(\"rnn_2d_44.csv\", index=False)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to develop an effective **Recurrent Neural Network**, the group decided to explore the concept of **Long Short Term Memory** (LSTM) networks. LSTMs are a type of RNN that are designed to handle sequential data pattern recognition. \n",
    "\n",
    "We consider this approach could be the most effective in order to classify the sounds, since continuous sounds or repetitive rythms are sequential. These time-dependant aspects are characteristics which LSTMs are capable of recognizing and \"remembering\" throughout training.\n",
    "\n",
    "In this case, each LSTM layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build this LSTM model, we decided to do some research and attempt to look into the topology suggested in a few published papers, before attempting to change and improve classification performance.\n",
    "\n",
    "The [first paper](https://dergi.neu.edu.tr/index.php/aiit/article/download/740/327/3147) suggests the following:\n",
    "\n",
    " - 22050Hz sample rate\n",
    " - 1 LSTM layer of size 128\n",
    " - followed another LSTM layer of size 64\n",
    " - SoftMax for prediction\n",
    " - 50 epochs\n",
    " - Adam optimization\n",
    " - Dropout Rate = 0.2\n",
    "\n",
    "This topology takes the large input vector size into consideration, meaning that it should maintain robustness and generally avoid major overfiting, while still allowing the model to identify patterns during an appropriate ammount of time. It also uses as sufficiently good sound quality for environment noise purposes, although we would like to verify if the difference in sound quality allows for better classification. A dropout rate of 0.2 (rate in which neurons are randomly disabled) is used in order to prevent overfitting.\n",
    "\n",
    "Overall, this model aims for efficiency and computation speed, in an effort to reduce training time while preserving model quality and performance, hence why we chose this as our starter model topology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[The second paper](https://annals-csis.org/Volume_18/drp/pdf/185.pdf) proposes almost identical topology (similar to most of the projects found), with the small changes of:\n",
    " - 44100Hz sample rate\n",
    " - 64 epochs\n",
    " - Dropout Rate = 0.25\n",
    "\n",
    "We expect this model to take longer to train (due to the higher number of epochs).\n",
    "It could lead to overfitting, which is why the **dropout rate** has also been **adjusted** to the slightly higher value of 0.25. If it is correctly trained, it **should demonstrate better results**, compared to the previous settings.\n",
    "\n",
    "Overall, it is a generally \"riskier\" model, and it was chosen to assess if the results of the combination combination of **increase in epochs** and **improved sound quality** could compensate the time consumed, render the training problems relativelly irrelevant if the generalization and classification abilities of the resulting model show significant improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We expect the **first model** to be trained faster due to inferior sound quality and lower epochs, in contrast with the **second model**, which could overfit but benefits from higher sound quality and dropout rate, potentially mitigating such problems, overall being preferable.\n",
    "\n",
    "Both results will be used as a type of foundation for the final topology decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REVIEW !!!\n",
    "\n",
    "If we have time, we can try:\n",
    "\n",
    " - 2 LSTM layers of size 128 (might take a while)\n",
    " - SoftMax for combination of 2D results\n",
    " - Sigmoid or Tanh for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the previously stated network topology and parameters, and using the available **tensorflow** and **keras** modules, we can start implementing an LSTM neural network, using available tools like:\n",
    "\n",
    " - `keras.Sequential()`: \n",
    " - `layers.LSTM`: \n",
    " - `layers.Dropout`:\n",
    " - `layers.TimeDistributed`: \n",
    " - `layers.Dropout`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a Neural Network generating function, which is based on the following concepts: \n",
    "\n",
    " - By ensuring that the input has a fixed shape and frequency, we can now use a Sequential keras model. This will allow for simple layer stacking and parameter configuration;\n",
    " - A dropout rate of 0.2 is employed in between each layer (with the exception of flatten and classification layers);\n",
    " - Time Distributed layers allow for easier handling of the extracted sequential features, by preserving the sequential time dimension features while reducing dimensionality;\n",
    " - In order to potentially improve our classifier's robustness, we decided to employ **l2 regularization** (or Lasso regularization), due to our dataset's reduced size;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easily configurable to generate with different architecture\n",
    "# Implicitly creates first model, explained above\n",
    "\n",
    "def generate_lstm(x, y, lstm1_size=128, lstm2_size=64, dropout=0.2):\n",
    "    \n",
    "    # Uses UrbanSounds8k split data shape\n",
    "    train_shape = (x, y) \n",
    "\n",
    "    # Initiaizes sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adds 2 LSTM layers of size 128 and 64, with a dropout rate of 0.2\n",
    "    model.add(layers.LSTM(lstm1_size,  input_shape = train_shape, return_sequences = True, activation='tanh'))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "\n",
    "    # The second layer does not need return sequences, \n",
    "    # since the next layer won't be another LSTM\n",
    "    model.add(layers.LSTM(lstm2_size, return_sequences = False, activation='tanh'))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "\n",
    "    # Suggested intermediate dense layer -> reduce dimensionality and preserve time sequential features\n",
    "    model.add(layers.TimeDistributed(layers.Dense(64, activation='tanh', kernel_regularizer = reg.l2(0.01))))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.TimeDistributed(layers.Dense(32, activation='tanh', kernel_regularizer = reg.l2(0.01))))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "\n",
    "    # /////////////////// NEEDS REVISION ///////////////////\n",
    "\n",
    "    # Last TimeDistributed produces 16 features/time step (thought dimensionality was appropriate)\n",
    "    model.add(layers.TimeDistributed(layers.Dense(16, activation='tanh', kernel_regularizer = reg.l2(0.01))))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "\n",
    "    # Flattens the last TimeDistributed outputs\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Dense output layer -> classification\n",
    "    model.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STILL NEEDS TO EBE COMPILED AND ADAM OPTIMIZED!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Many-to-Many LSTM for Sequence Prediction with TimeDistributed layers - [link](https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

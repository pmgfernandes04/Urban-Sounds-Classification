{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, we will be adressing our take on the problem using a **recurrent neural network**.\n",
    "\n",
    "We will begin  by importing the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm # progress bar on long runs\n",
    "from scipy.io import wavfile as wav\n",
    "import librosa\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "# allows to plot graphs inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per mentioned in the **project statement**, the target variable corresponds to the correct labeling of the sound. There are 10 different possible sounds in the dataset:\n",
    "\n",
    " - air conditioner\n",
    " - car horn\n",
    " - children playing\n",
    " - dog bark\n",
    " - drilling\n",
    " - engine idling\n",
    " - gun shot\n",
    " - jackhammer\n",
    " - siren\n",
    " - street music\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already find the `classID` column, which essentially represents each label as an integer, from 0 to 9:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classID: 0, class: air_conditioner\n",
      "classID: 1, class: car_horn\n",
      "classID: 2, class: children_playing\n",
      "classID: 3, class: dog_bark\n",
      "classID: 4, class: drilling\n",
      "classID: 5, class: engine_idling\n",
      "classID: 6, class: gun_shot\n",
      "classID: 7, class: jackhammer\n",
      "classID: 8, class: siren\n",
      "classID: 9, class: street_music\n"
     ]
    }
   ],
   "source": [
    "class_id_pairs = df[['classID', 'class']].drop_duplicates().sort_values(by=\"classID\")\n",
    "\n",
    "for index, row in class_id_pairs.iterrows():\n",
    "    print(f'classID: {row[\"classID\"]}, class: {row[\"class\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we can remove the last column and begin working with our dataset, which we already determined is slightly unbalanced for the `car_horn` and `gunshot` values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3\n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2\n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2\n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2\n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['class'],inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "The **librosa** library has a built-in method for feature extraction, called [Mel-Frequency Cepstral Coefficients](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum), that summarises the frequency distribution across the time window.\n",
    "\n",
    "In order to build the new dataset, we developed the following functions, which are capable of extracting **1D or 2D** features.\n",
    "\n",
    "These feature extractor functions will represent the frequencies found in the wav files as **np arrays**, while using MFCCs in order to obtain features similar to the way humans perceive sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the mean from the Time axis\n",
    "def features_extractor_1D(file):\n",
    "    audio, sample_rate = librosa.load(file) \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0) \n",
    "    return mfccs_scaled_features\n",
    "\n",
    "# Uses both Time and Frequency axis\n",
    "def features_extractor_2D(file):\n",
    "    audio, sample_rate = librosa.load(file) \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    return mfccs_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to transform audio files into usable data types, we must associate each numpy array to their respective entry inside the df dataframe.\n",
    "\n",
    "This will allow for important pre-processing steps to be applied accordingly, as well as proper Neural Network training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/8732 [00:00<?, ?row/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 8732/8732 [02:03<00:00, 70.86row/s] \n"
     ]
    }
   ],
   "source": [
    "# Identify path containing all folds\n",
    "audio_dataset_path='../UrbanSound8K/audio/'\n",
    "extracted_features1=[]\n",
    "extracted_features2=[]\n",
    "\n",
    "'''\n",
    "\n",
    "# Iterates over all original dataframe rows (predicts approximate runtime)\n",
    "for index_num,row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\", unit=\"row\"):\n",
    "    # Identifies wav file name, concatenates to respective fold: accesses original .wav file\n",
    "    #file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'\\\\',str(row[\"slice_file_name\"]))\n",
    "    \n",
    "    # Adds associated sound label\n",
    "    final_class_labels=row[\"classID\"]\n",
    "    \n",
    "\n",
    "    data1=features_extractor_1D(file_name) \n",
    "    extracted_features1.append([data1,final_class_labels])\n",
    "\n",
    "    data2=features_extractor_2D(file_name) \n",
    "    extracted_features2.append([data2,final_class_labels])\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# Convert extracted_features to Pandas dataframe\n",
    "df_1d =pd.DataFrame(extracted_features1,columns=['feature','class'])\n",
    "df_2d =pd.DataFrame(extracted_features2,columns=['feature','class'])\n",
    "\n",
    "df_1d.to_csv(\"rnn_1d.csv\", index=False)\n",
    "df_2d.to_csv(\"rnn_2d.csv\", index=False)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librosa extracts MFCCs on different scales for different .wav files. This is due to the fact that lower frequencies are emphasized during this process, potentially creating bias issues as a consequence of heterogeneous distributions of frequencies throughout each file.\n",
    "\n",
    "To address this issue, we can apply feature scaling to the new dataframes, in order to improve data quality for our modeling purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First arrray of the first entry in the 2D dataset: \n",
      " [-332.03876  -169.58775   -90.24683   -56.92349   -40.27587   -50.544167\n",
      "  -99.22394  -159.67033  -215.6434   -267.75623  -316.28625  -355.39624\n",
      " -390.08435  -423.43994 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Iterates over all original dataframe rows (predicts approximate runtime)\\nfor index_num,row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\", unit=\"row\"):\\n    # Get the \"features\" array for the current row\\n    features_array = row[\\'feature\\']\\n\\n    # Ensure the features_array is a 2D array (in case it is 1D)\\n    # If it\\'s a 1D array of shape (40,) for example, reshape it into (40, 1) for scaling\\n    if isinstance(features_array, np.ndarray):  # Check if the element is a numpy array\\n        if features_array.ndim == 1:\\n            # Reshape the 1D array to 2D for scaling\\n            features_array = features_array.reshape(-1, 1)\\n        \\n        # Apply Min-Max scaling to the array\\n        scaled_features = scaler.fit_transform(features_array).flatten()  # Flatten to maintain 1D structure after scaling\\n        \\n        # Update the \"features\" column with the scaled array (in-place)\\n        df.at[index_num, \\'feature\\'] = scaled_features\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses sklearn's MinMax scaler, rescales values to be in a range of [0,1]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# /////////////////// NEEDS REVISION ///////////////////\n",
    "\n",
    "# example = df_2d.iloc[0][\"feature\"][0]\n",
    "# print(\"First arrray of the first entry in the 2D dataset: \\n\", example)\n",
    "\n",
    "'''\n",
    "# Iterates over all original dataframe rows (predicts approximate runtime)\n",
    "for index_num,row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\", unit=\"row\"):\n",
    "    # Get the \"features\" array for the current row\n",
    "    features_array = row['feature']\n",
    "\n",
    "    # Ensure the features_array is a 2D array (in case it is 1D)\n",
    "    # If it's a 1D array of shape (40,) for example, reshape it into (40, 1) for scaling\n",
    "    if isinstance(features_array, np.ndarray):  # Check if the element is a numpy array\n",
    "        if features_array.ndim == 1:\n",
    "            # Reshape the 1D array to 2D for scaling\n",
    "            features_array = features_array.reshape(-1, 1)\n",
    "        \n",
    "        # Apply Min-Max scaling to the array\n",
    "        scaled_features = scaler.fit_transform(features_array).flatten()  # Flatten to maintain 1D structure after scaling\n",
    "        \n",
    "        # Update the \"features\" column with the scaled array (in-place)\n",
    "        df.at[index_num, 'feature'] = scaled_features\n",
    "'''\n",
    "# /////////////////// NEEDS REVISION ///////////////////"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since not every `.wav` file is 4 seconds long, we will apply **zero-padding** to ensure that all files meet this requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /////////////////// NEEDS REVISION ///////////////////"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to develop an effective **Recurrent Neural Network**, the group decided to explore the concept of **Long Short Term Memory** (LSTM) networks. LSTMs are a type of RNN that are designed to handle sequential data pattern recognition. \n",
    "\n",
    "We consider this approach could be the most effective in order to classify the sounds, since continuous sounds or repetitive rythms are sequential. These time-dependant aspects are characteristics which LSTMs are capable of recognizing and \"remembering\" throughout training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

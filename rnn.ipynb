{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, we will be adressing our take on the problem using a **recurrent neural network**.\n",
    "\n",
    "We will begin  by importing the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm # progress bar on long runs\n",
    "from scipy.io import wavfile as wav\n",
    "import librosa\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras import regularizers as reg\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per mentioned in the **project statement**, the target variable corresponds to the correct labeling of the sound. There are 10 different possible sounds in the dataset:\n",
    "\n",
    " - air conditioner\n",
    " - car horn\n",
    " - children playing\n",
    " - dog bark\n",
    " - drilling\n",
    " - engine idling\n",
    " - gun shot\n",
    " - jackhammer\n",
    " - siren\n",
    " - street music\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already find the `classID` column, which essentially represents each label as an integer, from 0 to 9:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classID: 0, class: air_conditioner\n",
      "classID: 1, class: car_horn\n",
      "classID: 2, class: children_playing\n",
      "classID: 3, class: dog_bark\n",
      "classID: 4, class: drilling\n",
      "classID: 5, class: engine_idling\n",
      "classID: 6, class: gun_shot\n",
      "classID: 7, class: jackhammer\n",
      "classID: 8, class: siren\n",
      "classID: 9, class: street_music\n"
     ]
    }
   ],
   "source": [
    "class_id_pairs = df[['classID', 'class']].drop_duplicates().sort_values(by=\"classID\")\n",
    "\n",
    "for index, row in class_id_pairs.iterrows():\n",
    "    print(f'classID: {row[\"classID\"]}, class: {row[\"class\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we can remove the last column and begin working with our dataset, which we already determined is slightly unbalanced for the `car_horn` and `gunshot` values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3\n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2\n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2\n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2\n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['class'],inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since not every `.wav` file is 4 seconds long, we will apply **zero-padding** to ensure that all files meet this requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /////////////////// NEEDS REVISION ///////////////////\n",
    "\n",
    "# I don't understand how to do this in the np arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve dataset consistency and traning data quality, we also decided to create 2 new datasets: \n",
    "\n",
    " - `df_22`: resamples data to 22050Hz\n",
    " - `df_44`: resamples data to 44100Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEED HELP\n",
    "\n",
    "not sure if i should do resampling before feature extraction or during feature extraction. during feature extraction would probably help automate different quality sound extraction. but it will depend on the 0-padding order aswell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librosa extracts MFCCs on different scales for different .wav files. This is due to the fact that lower frequencies are emphasized during this process, potentially creating bias issues as a consequence of heterogeneous distributions of frequencies throughout each file.\n",
    "\n",
    "To address this issue, we can apply feature scaling to the new dataframes, in order to improve data quality for our modeling purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Iterates over all original dataframe rows (predicts approximate runtime)\\nfor index_num,row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\", unit=\"row\"):\\n    # Get the \"features\" array for the current row\\n    features_array = row[\\'feature\\']\\n\\n    # Ensure the features_array is a 2D array (in case it is 1D)\\n    # If it\\'s a 1D array of shape (40,) for example, reshape it into (40, 1) for scaling\\n    if isinstance(features_array, np.ndarray):  # Check if the element is a numpy array\\n        if features_array.ndim == 1:\\n            # Reshape the 1D array to 2D for scaling\\n            features_array = features_array.reshape(-1, 1)\\n        \\n        # Apply Min-Max scaling to the array\\n        scaled_features = scaler.fit_transform(features_array).flatten()  # Flatten to maintain 1D structure after scaling\\n        \\n        # Update the \"features\" column with the scaled array (in-place)\\n        df.at[index_num, \\'feature\\'] = scaled_features\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses sklearn's MinMax scaler, rescales values to be in a range of [0,1]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# /////////////////// NEEDS REVISION ///////////////////\n",
    "\n",
    "# example = df_2d.iloc[0][\"feature\"][0]\n",
    "# print(\"First arrray of the first entry in the 2D dataset: \\n\", example)\n",
    "\n",
    "'''\n",
    "# Iterates over all original dataframe rows (predicts approximate runtime)\n",
    "for index_num,row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\", unit=\"row\"):\n",
    "    # Get the \"features\" array for the current row\n",
    "    features_array = row['feature']\n",
    "\n",
    "    # Ensure the features_array is a 2D array (in case it is 1D)\n",
    "    # If it's a 1D array of shape (40,) for example, reshape it into (40, 1) for scaling\n",
    "    if isinstance(features_array, np.ndarray):  # Check if the element is a numpy array\n",
    "        if features_array.ndim == 1:\n",
    "            # Reshape the 1D array to 2D for scaling\n",
    "            features_array = features_array.reshape(-1, 1)\n",
    "        \n",
    "        # Apply Min-Max scaling to the array\n",
    "        scaled_features = scaler.fit_transform(features_array).flatten()  # Flatten to maintain 1D structure after scaling\n",
    "        \n",
    "        # Update the \"features\" column with the scaled array (in-place)\n",
    "        df.at[index_num, 'feature'] = scaled_features\n",
    "'''\n",
    "# /////////////////// NEEDS REVISION ///////////////////"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "The **librosa** library has a built-in method for feature extraction, called [Mel-Frequency Cepstral Coefficients](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum), that summarises the frequency distribution across the time window.\n",
    "\n",
    "In order to build the new dataset, we developed the following functions, which are capable of extracting **1D or 2D** features.\n",
    "\n",
    "These feature extractor functions will represent the frequencies found in the wav files as **np arrays**, while using MFCCs in order to obtain features similar to the way humans perceive sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the mean from the Time axis, uses file sample rate\n",
    "def features_extractor_1D(file):\n",
    "    audio, sample_rate = librosa.load(file) \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0) \n",
    "    return mfccs_scaled_features\n",
    "\n",
    "# Uses both Time and Frequency axis, custom sample rate\n",
    "def features_extractor_2D(file, sample_rate, path=True):\n",
    "    if path: audio, _ = librosa.load(file) \n",
    "    else: audio = file\n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    return mfccs_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to transform audio files into usable data types, we must associate each numpy array to their respective entry inside the df dataframe.\n",
    "\n",
    "This will allow for important pre-processing steps to be applied accordingly, as well as proper Neural Network training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to identify files with duration < 4s and apply zero padding for consistency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(file_path, target_sr=44100, target_length=4):\n",
    "    \"\"\"\n",
    "    Loads an audio file, resamples it to the target sample rate,\n",
    "    and pads or trims it to the target length.\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(file_path, sr=target_sr)\n",
    "    target_samples = int(target_length * sr)\n",
    "    \n",
    "    if len(y) > target_samples:\n",
    "        # Trim the audio to the target length\n",
    "        y = y[:target_samples]\n",
    "    else:\n",
    "        # Pad the audio with zeros (silence) to reach the target length\n",
    "        padding = target_samples - len(y)\n",
    "        y = np.pad(y, (0, padding), 'constant')\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Iterates over all original dataframe rows (predicts approximate runtime)\\nfor index_num,row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\", unit=\"row\"):\\n    # Identifies wav file name, concatenates to respective fold: accesses original .wav file\\n    #file_name = os.path.join(os.path.abspath(audio_dataset_path),\\'fold\\'+str(row[\"fold\"])+\\'/\\',str(row[\"slice_file_name\"]))\\n    file_name = os.path.join(os.path.abspath(audio_dataset_path),\\'fold\\'+str(row[\"fold\"])+\\'\\\\\\',str(row[\"slice_file_name\"]))\\n    \\n    # Adds associated sound label\\n    final_class_labels=row[\"classID\"]\\n\\n    y = zero_padding(file_name)\\n\\n    # 22050Hz sample rate\\n    data1=features_extractor_2D(y, 22050, False) \\n    extracted_features22.append([data1,final_class_labels])\\n\\n    # 44100hHz sample rate\\n    data2=features_extractor_2D(y, 44100, False) \\n    extracted_features44.append([data2,final_class_labels])\\n    \\n\\n# Convert extracted_features to Pandas dataframe\\ndf_22 =pd.DataFrame(extracted_features22,columns=[\\'feature\\',\\'class\\'])\\ndf_44 =pd.DataFrame(extracted_features44,columns=[\\'feature\\',\\'class\\'])\\n\\n\\ndf_22.to_pickle(\"rnn_2d_22.pkl\")\\ndf_44.to_pickle(\"rnn_2d_44.pkl\")\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify path containing all folds\n",
    "audio_dataset_path='../UrbanSound8K/audio/'\n",
    "extracted_features22=[]\n",
    "extracted_features44=[]\n",
    "\n",
    "\n",
    "'''\n",
    "# Iterates over all original dataframe rows (predicts approximate runtime)\n",
    "for index_num,row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\", unit=\"row\"):\n",
    "    # Identifies wav file name, concatenates to respective fold: accesses original .wav file\n",
    "    #file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'\\\\',str(row[\"slice_file_name\"]))\n",
    "    \n",
    "    # Adds associated sound label\n",
    "    final_class_labels=row[\"classID\"]\n",
    "\n",
    "    y = zero_padding(file_name)\n",
    "\n",
    "    # 22050Hz sample rate\n",
    "    data1=features_extractor_2D(y, 22050, False) \n",
    "    extracted_features22.append([data1,final_class_labels])\n",
    "\n",
    "    # 44100hHz sample rate\n",
    "    data2=features_extractor_2D(y, 44100, False) \n",
    "    extracted_features44.append([data2,final_class_labels])\n",
    "    \n",
    "\n",
    "# Convert extracted_features to Pandas dataframe\n",
    "df_22 =pd.DataFrame(extracted_features22,columns=['feature','class'])\n",
    "df_44 =pd.DataFrame(extracted_features44,columns=['feature','class'])\n",
    "\n",
    "\n",
    "df_22.to_pickle(\"rnn_2d_22.pkl\")\n",
    "df_44.to_pickle(\"rnn_2d_44.pkl\")\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding common pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original [UrbanSound8k](https://urbansounddataset.weebly.com/urbansound8k.html) website features a section outlined specifically for **cross validation**.\n",
    "\n",
    "These rules emphasize the specific creation of 10 folds, following the identifiers present in the `fold` column from the original csv. It is explained that different folds display different levels of classification difficulty, which could potentially invalidate model efficiency if not performed correctly. \n",
    "\n",
    "Since both the `df_44` and `df_22` dataframes were created using the row order of the df_dataframe, we know that the `fold` column will be correctly aligned with the rest of the data. This means that we can select specific rows and assing them to their respective fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-470.67493, -378.39322, -278.98053, -196.987...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-529.98474, -505.75565, -515.84247, -514.398...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-538.82623, -519.2845, -542.29865, -549.1047...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-506.23874, -482.36063, -478.73837, -473.929...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-533.9983, -507.45364, -511.3955, -515.28906...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature  class  fold\n",
       "0  [[-470.67493, -378.39322, -278.98053, -196.987...      3     5\n",
       "1  [[-529.98474, -505.75565, -515.84247, -514.398...      2     5\n",
       "2  [[-538.82623, -519.2845, -542.29865, -549.1047...      2     5\n",
       "3  [[-506.23874, -482.36063, -478.73837, -473.929...      2     5\n",
       "4  [[-533.9983, -507.45364, -511.3955, -515.28906...      2     5"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avoids constant running of feature extraction\n",
    "df_44 = pd.read_pickle('../rnn_2d_44.pkl')\n",
    "\n",
    "df_44['fold'] = df['fold']\n",
    "\n",
    "df_44.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to develop an effective **Recurrent Neural Network**, the group decided to explore the concept of **Long Short Term Memory** (LSTM) networks. LSTMs are a type of RNN that are designed to handle sequential data pattern recognition. \n",
    "\n",
    "We consider this approach could be the most effective in order to classify the sounds, since continuous sounds or repetitive rythms are sequential. These time-dependant aspects are characteristics which LSTMs are capable of recognizing and \"remembering\" throughout training.\n",
    "\n",
    "In this case, each LSTM layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build this LSTM model, we decided to do some research and attempt to look into the topology suggested in a few published papers, before attempting to change and improve classification performance.\n",
    "\n",
    "The [first paper](https://dergi.neu.edu.tr/index.php/aiit/article/download/740/327/3147) suggests the following:\n",
    "\n",
    " - 22050Hz sample rate\n",
    " - 1 LSTM layer of size 128\n",
    " - followed another LSTM layer of size 64\n",
    " - SoftMax for prediction\n",
    " - 50 epochs\n",
    " - Adam optimization\n",
    " - Dropout Rate = 0.2\n",
    "\n",
    "This topology takes the large input vector size into consideration, meaning that it should maintain robustness and generally avoid major overfiting, while still allowing the model to identify patterns during an appropriate ammount of time. It also uses as sufficiently good sound quality for environment noise purposes, although we would like to verify if the difference in sound quality allows for better classification. A dropout rate of 0.2 (rate in which neurons are randomly disabled) is used in order to prevent overfitting.\n",
    "\n",
    "Overall, this model aims for efficiency and computation speed, in an effort to reduce training time while preserving model quality and performance, hence why we chose this as our starter model topology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[The second paper](https://annals-csis.org/Volume_18/drp/pdf/185.pdf) proposes almost identical topology (similar to most of the projects found), with the small changes of:\n",
    " - 44100Hz sample rate\n",
    " - 64 epochs\n",
    " - Dropout Rate = 0.25\n",
    "\n",
    "We expect this model to take longer to train (due to the higher number of epochs).\n",
    "It could lead to overfitting, which is why the **dropout rate** has also been **adjusted** to the slightly higher value of 0.25. If it is correctly trained, it **should demonstrate better results**, compared to the previous settings.\n",
    "\n",
    "Overall, it is a generally \"riskier\" model, and it was chosen to assess if the results of the combination combination of **increase in epochs** and **improved sound quality** could compensate the time consumed, render the training problems relativelly irrelevant if the generalization and classification abilities of the resulting model show significant improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We expect the **first model** to be trained faster due to inferior sound quality and lower epochs, in contrast with the **second model**, which could overfit but benefits from higher sound quality and dropout rate, potentially mitigating such problems, overall being preferable.\n",
    "\n",
    "Both results will be used as a type of foundation for the final topology decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REVIEW !!!\n",
    "\n",
    "If we have time, we can try:\n",
    "\n",
    " - 2 LSTM layers of size 128 (might take a while)\n",
    " - SoftMax for combination of 2D results\n",
    " - Sigmoid or Tanh for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the previously stated network topology and parameters, and using the available **tensorflow** and **keras** modules, we can start implementing an LSTM neural network, using available tools like:\n",
    "\n",
    " - `keras.Sequential()`: \n",
    " - `layers.LSTM`: \n",
    " - `layers.Dropout`:\n",
    " - `layers.TimeDistributed`: \n",
    " - `layers.Dropout`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a Neural Network generating function, which is based on the following concepts: \n",
    "\n",
    " - By ensuring that the input has a fixed shape and frequency, we can now use a Sequential keras model. This will allow for simple layer stacking and parameter configuration;\n",
    " - A dropout rate of 0.2 is employed in between each layer (with the exception of flatten and classification layers);\n",
    " - Time Distributed layers allow for easier handling of the extracted sequential features, by preserving the sequential time dimension features while reducing dimensionality;\n",
    " - In order to potentially improve our classifier's robustness, we decided to employ **l2 regularization** (or Lasso regularization), due to our dataset's reduced size;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easily configurable to generate with different architecture\n",
    "# Implicitly creates first model, explained above\n",
    "\n",
    "def generate_lstm(train_shape, lstm1_size=128, lstm2_size=64, dropout=0.2):\n",
    "    # Initiaizes sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adds 2 LSTM layers of size 128 and 64, with a dropout rate of 0.2\n",
    "    model.add(layers.LSTM(lstm1_size,  input_shape = train_shape, return_sequences = True, activation='tanh'))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.LSTM(lstm2_size, return_sequences = True, activation='tanh'))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "\n",
    "    # Suggested intermediate dense layer -> reduce dimensionality and preserve time sequential features\n",
    "    model.add(layers.TimeDistributed(layers.Dense(64, activation='tanh', kernel_regularizer = reg.l2(0.01))))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.TimeDistributed(layers.Dense(32, activation='tanh', kernel_regularizer = reg.l2(0.01))))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "\n",
    "    # /////////////////// NEEDS REVISION ///////////////////\n",
    "\n",
    "    # Last TimeDistributed produces 16 features/time step (thought dimensionality was appropriate)\n",
    "    model.add(layers.TimeDistributed(layers.Dense(16, activation='tanh', kernel_regularizer = reg.l2(0.01))))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "\n",
    "    # Flattens the last TimeDistributed outputs\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Dense output layer -> classification\n",
    "    model.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define X and y, we need to use 9 folds for building (X_train, y_train) and the remaining fold as for building (X_test, y_test), in the 10 different possible ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, i, values_log=False, full_log=False,):\n",
    "    train_dataframe = df[df[\"fold\"] != i]  # Rows where \"fold\" is not i -> different training combinations\n",
    "    test_dataframe = df[df[\"fold\"] == i]  # Rows where \"fold\" is i -> different testing combinations\n",
    "\n",
    "    # Split the dataset into training features and class\n",
    "    X_train = np.array(train_dataframe['feature'].tolist())\n",
    "    y_train = np.array(train_dataframe['class'].tolist())\n",
    "\n",
    "    # Split the dataset into testing features and class\n",
    "    X_test = np.array(test_dataframe['feature'].tolist())\n",
    "    y_test = np.array(test_dataframe['class'].tolist())\n",
    "\n",
    "    # Labels must be one hot encoded for the LSTM output layer\n",
    "    y_train_encoded = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "    y_test_encoded = keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "\n",
    "    if full_log: \n",
    "        print(f\"Test folder: {i}\")\n",
    "        print(f\"X_train shape: {X_train.shape} - (Samples: {X_train.shape[0]}, Time Steps: {X_train.shape[1]}, Features: {X_train.shape[2]})\")\n",
    "        print(f\"y_train shape (before encoding): {y_train.shape} - (Samples: {y_train.shape[0]})\")\n",
    "        print(f\"y_train_encoded shape (after encoding): {y_train_encoded.shape} - (Samples: {y_train_encoded.shape[0]}, Classes: {y_train_encoded.shape[1]})\")\n",
    "        print(f\"X_test shape: {X_test.shape} - (Samples: {X_test.shape[0]}, Time Steps: {X_test.shape[1]}, Features: {X_test.shape[2]})\")\n",
    "        print(f\"y_test shape (before encoding): {y_test.shape} - (Samples: {y_test.shape[0]})\")\n",
    "        print(f\"y_test_encoded shape (after encoding): {y_test_encoded.shape} - (Samples: {y_test_encoded.shape[0]}, Classes: {y_test_encoded.shape[1]})\")\n",
    "        print(\"-\" * 85)\n",
    "\n",
    "    elif values_log:\n",
    "        print(f\"X_train: {X_train.shape}\")\n",
    "        print(f\"y_train_encoded: {y_train_encoded.shape}\")\n",
    "        print(f\"X_test: {X_test.shape}\")\n",
    "        print(f\"y_test_encoded: {y_test_encoded.shape}\")\n",
    "        print(\"-\" * 85)\n",
    "\n",
    "    return X_train, y_train_encoded, X_test, y_test_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our train and test data shapes for every combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (7859, 40, 345)\n",
      "y_train_encoded: (7859, 10)\n",
      "X_test: (873, 40, 345)\n",
      "y_test_encoded: (873, 10)\n",
      "-------------------------------------------------------------------------------------\n",
      "X_train: (7844, 40, 345)\n",
      "y_train_encoded: (7844, 10)\n",
      "X_test: (888, 40, 345)\n",
      "y_test_encoded: (888, 10)\n",
      "-------------------------------------------------------------------------------------\n",
      "X_train: (7807, 40, 345)\n",
      "y_train_encoded: (7807, 10)\n",
      "X_test: (925, 40, 345)\n",
      "y_test_encoded: (925, 10)\n",
      "-------------------------------------------------------------------------------------\n",
      "X_train: (7742, 40, 345)\n",
      "y_train_encoded: (7742, 10)\n",
      "X_test: (990, 40, 345)\n",
      "y_test_encoded: (990, 10)\n",
      "-------------------------------------------------------------------------------------\n",
      "X_train: (7796, 40, 345)\n",
      "y_train_encoded: (7796, 10)\n",
      "X_test: (936, 40, 345)\n",
      "y_test_encoded: (936, 10)\n",
      "-------------------------------------------------------------------------------------\n",
      "X_train: (7909, 40, 345)\n",
      "y_train_encoded: (7909, 10)\n",
      "X_test: (823, 40, 345)\n",
      "y_test_encoded: (823, 10)\n",
      "-------------------------------------------------------------------------------------\n",
      "X_train: (7894, 40, 345)\n",
      "y_train_encoded: (7894, 10)\n",
      "X_test: (838, 40, 345)\n",
      "y_test_encoded: (838, 10)\n",
      "-------------------------------------------------------------------------------------\n",
      "X_train: (7926, 40, 345)\n",
      "y_train_encoded: (7926, 10)\n",
      "X_test: (806, 40, 345)\n",
      "y_test_encoded: (806, 10)\n",
      "-------------------------------------------------------------------------------------\n",
      "X_train: (7916, 40, 345)\n",
      "y_train_encoded: (7916, 10)\n",
      "X_test: (816, 40, 345)\n",
      "y_test_encoded: (816, 10)\n",
      "-------------------------------------------------------------------------------------\n",
      "X_train: (7895, 40, 345)\n",
      "y_train_encoded: (7895, 10)\n",
      "X_test: (837, 40, 345)\n",
      "y_test_encoded: (837, 10)\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    X_train, y_train, X_test, y_test = train_test_split(df_44, i, values_log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM's input dimensions are determined by the shape of our data. As you can confirm in the print statement above, our shapes' first index are not constant in each fold, but they display a consistent structure:\n",
    "\n",
    "  - `batch_size`: Number of samples in the set (7859, 7844...)\n",
    "  - `time_steps`: Number of time steps in the data (always 40)\n",
    "  - `n_features`: Number of features at each time step (always 345)\n",
    "\n",
    "Furthermore, we must build our model using the following:\n",
    "\n",
    "  - **input_shape** = (time_steps, n_features)\n",
    "\n",
    "Which directly corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (40, 345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And creates models with the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_image = generate_lstm(input_shape)\n",
    "\n",
    "keras.utils.plot_model(model_for_image, \"model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 23ms/step - accuracy: 0.2829 - loss: 2.9644 - val_accuracy: 0.5135 - val_loss: 1.8082\n",
      "Epoch 2/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.5647 - loss: 1.6249 - val_accuracy: 0.4977 - val_loss: 1.4790\n",
      "Epoch 3/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.6458 - loss: 1.2129 - val_accuracy: 0.5428 - val_loss: 1.3006\n",
      "Epoch 4/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.6893 - loss: 1.0229 - val_accuracy: 0.5417 - val_loss: 1.3680\n",
      "Epoch 5/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.7183 - loss: 0.9247 - val_accuracy: 0.5338 - val_loss: 1.4088\n",
      "Epoch 6/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.7586 - loss: 0.7886 - val_accuracy: 0.5225 - val_loss: 1.3959\n",
      "Epoch 7/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.7775 - loss: 0.7299 - val_accuracy: 0.5248 - val_loss: 1.4415\n",
      "Epoch 8/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.7995 - loss: 0.6847 - val_accuracy: 0.5833 - val_loss: 1.4233\n",
      "Epoch 9/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.8364 - loss: 0.5704 - val_accuracy: 0.5282 - val_loss: 1.5071\n",
      "Epoch 10/10\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.8547 - loss: 0.5068 - val_accuracy: 0.5394 - val_loss: 1.6263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19b5bafab40>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = train_test_split(df_44, 2)\n",
    "\n",
    "# Compile the model\n",
    "model_for_image.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_for_image.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Introduction to Extraction of Data from the UrbanSound8k dataset - [part1](https://www.youtube.com/watch?v=mHPpCXqQd7Y), [part2](https://www.youtube.com/watch?v=4F-cwOkMdTE)\n",
    "\n",
    "Many-to-Many LSTM for Sequence Prediction with TimeDistributed layers - [link](https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

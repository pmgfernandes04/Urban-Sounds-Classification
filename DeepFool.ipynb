{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Introduction**\n",
    "\n",
    "Deep learning models have achieved significant success across various domains, including image recognition, natural language processing, and audio analysis. Despite their impressive accuracy, these models are susceptible to **adversarial examples**â€”small, often imperceptible perturbations that can cause misclassifications. Understanding and mitigating these vulnerabilities are critical for improving model robustness.\n",
    "\n",
    "The **DeepFool algorithm**, proposed by Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard, systematically computes the smallest perturbation required to alter a model's prediction. This notebook employs DeepFool to evaluate the robustness of two architectures trained on the **UrbanSound8K dataset**: a **Convolutional Neural Network (CNN)** and a **Recurrent Neural Network (RNN)**.\n",
    "\n",
    "---\n",
    "\n",
    "### **DeepFool Algorithm**\n",
    "\n",
    "The DeepFool algorithm iteratively linearizes a classifier's decision boundaries to compute the smallest perturbation $ r $ required to change the predicted label of an input $x$.\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Input: Classifier $f$, example $x$, overshoot parameter $ \\eta \\ $, and maximum iterations $T $.\n",
    "   - Predicted label:\n",
    "     $$\n",
    "     \\hat{k}(x) = \\arg\\max_k f_k(x),\n",
    "     $$\n",
    "     where $f_k(x)$ is the classifier's probability for class $k $.\n",
    "\n",
    "2. **Iteration**:\n",
    "   - Compute the gradients of the classifier output with respect to $x$ for all classes.\n",
    "   - Determine the minimal perturbation $r$ required to cross the decision boundary:\n",
    "     $$\n",
    "     r = \\frac{\\left| f_k(x) - f_{\\hat{k}(x)}(x) \\right|}{\\left\\| \\nabla f_k(x) - \\nabla f_{\\hat{k}(x)}(x) \\right\\|_2} \\cdot \\frac{\\nabla f_k(x) - \\nabla f_{\\hat{k}(x)}(x)}{\\left\\| \\nabla f_k(x) - \\nabla f_{\\hat{k}(x)}(x) \\right\\|_2}.\n",
    "     $$\n",
    "   - Update $ x $ with the perturbation $ r $:\n",
    "     $$\n",
    "     x \\gets x + r.\n",
    "     $$\n",
    "\n",
    "3. **Stopping Condition**:\n",
    "   - The algorithm stops when $ \\hat{k}(x + r) \\neq \\hat{k}(x) $ or the maximum number of iterations $ T $ is reached.\n",
    "\n",
    "4. **Output**:\n",
    "   - Return the minimal perturbation $ r $, the number of iterations, and the new predicted label.\n",
    "\n",
    "---\n",
    "\n",
    "### **Robustness Metric**\n",
    "\n",
    "The robustness of a classifier $ f $ is defined as the expected relative norm of the minimal perturbation $ r $ with respect to the input $ x $:\n",
    "$$\n",
    "\\rho_{\\text{adv}}(f) = \\mathbb{E}_{x \\sim \\mathcal{X}_{\\text{test}}} \\left[ \\frac{\\|r\\|_2}{\\|x\\|_2} \\right].\n",
    "$$\n",
    "\n",
    "In practice, this expectation is approximated by the mean over all test examples:\n",
    "$$\n",
    "\\rho_{\\text{adv}}(f) \\approx \\frac{1}{|\\mathcal{X}_{\\text{test}}|} \\sum_{x \\in \\mathcal{X}_{\\text{test}}} \\frac{\\|r\\|_2}{\\|x\\|_2}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Notebook Objectives**\n",
    "\n",
    "1. **Apply DeepFool to CNN and RNN Models**:\n",
    "   - Evaluate minimal adversarial perturbations for both architectures trained on the UrbanSound8K dataset.\n",
    "\n",
    "2. **Measure Robustness**:\n",
    "   - Compute the robustness metric $rho_{\\text{adv}}$ for both models across 10 cross-validation folds.\n",
    "\n",
    "3. **Compare Results**:\n",
    "   - Analyze differences in robustness between CNN and RNN architectures to identify which model demonstrates greater resistance to adversarial perturbations.\n",
    "\n",
    "This notebook provides insights into the adversarial vulnerabilities of CNN and RNN models, offering guidance for developing more robust deep learning systems."
   ],
   "id": "abf8853392ced647"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports and Setup",
   "id": "9d3a4bd0e762f8e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T18:46:07.801417Z",
     "start_time": "2024-12-01T18:45:35.752674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "import keras\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Constants\n",
    "SAMPLE_RATE = 22050\n",
    "HOP_LENGTH = round(SAMPLE_RATE * 0.0125)\n",
    "TIME_SIZE = 4 * SAMPLE_RATE // HOP_LENGTH + 1\n",
    "TARGET_WIDTH = 320  # Width for 2D features\n",
    "NUM_CLASSES = 10  # Number of classes in the dataset\n"
   ],
   "id": "f51623f7481b6d37",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Preparation",
   "id": "4497c076c507e61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This cell loads pre-extracted features from the UrbanSound8K dataset, organizes them into a DataFrame, and prepares both 2D (e.g., mel spectrograms, chroma features) and 1D (e.g., spectral rolloff, zero-crossing rate) features for model input.",
   "id": "9258856edd8b391f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T18:46:23.503413Z",
     "start_time": "2024-12-01T18:46:07.806419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load pre-extracted features\n",
    "with open('urbansound8k_features.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Create a DataFrame for easy manipulation\n",
    "features_df = pd.DataFrame(data)\n",
    "\n",
    "# Prepare 2D and 1D features\n",
    "X_mel = np.array(features_df['mel_spec'].tolist())\n",
    "X_mfcc = np.array(features_df['mfccs'].tolist())\n",
    "X_chroma = np.array(features_df['chroma'].tolist())\n",
    "X_contrast = np.array(features_df['spectral_contrast'].tolist())\n",
    "X_rolloff = np.array(features_df['spectral_rolloff'].tolist())\n",
    "X_zcr = np.array(features_df['zero_crossing_rate'].tolist())\n",
    "y = np.array(features_df['label'].tolist())\n",
    "\n",
    "# Reshape 1D features\n",
    "X_rolloff = X_rolloff.reshape(X_rolloff.shape[0], -1)\n",
    "X_zcr = X_zcr.reshape(X_zcr.shape[0], -1)"
   ],
   "id": "377b5566a50dffb9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Supporting Functions",
   "id": "c1ce377f4618894a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Description:**\n",
    "\n",
    "This code implements the **DeepFool algorithm** for computing minimal adversarial perturbations on a multi-input deep learning model. It includes the following functions:\n",
    "\n",
    "1. **`get_gradient`**:  \n",
    "   Computes the gradient of the \\(k\\)-th output of the model with respect to its multi-input data. The function is specifically designed to handle models with multiple input features by treating inputs as dictionaries of tensors. This is essential for determining the direction and magnitude of perturbations required to fool the model.\n",
    "\n",
    "2. **`deepfool`**:  \n",
    "   Implements the DeepFool algorithm to iteratively find the smallest perturbation \\(r\\) that changes the model's predicted label.  \n",
    "   - **Supports multi-input models** by processing inputs as dictionaries, where each key corresponds to a specific feature type (e.g., `input_1`, `input_2`).\n",
    "   - **Inputs**:\n",
    "     - `model`: The multi-input neural network.\n",
    "     - `x0`: The original input as a dictionary of tensors.\n",
    "     - `eta`: Overshoot parameter to adjust perturbation size.\n",
    "     - `max_iter`: Maximum number of iterations allowed.\n",
    "     - `num_classes`: Number of classes in the classification task.\n",
    "   - **Outputs**:\n",
    "     - `r_sum`: The cumulative perturbation applied to the input.\n",
    "     - `loop_i`: Number of iterations performed.\n",
    "     - `label_xi`: The new predicted label after perturbation.\n",
    "\n",
    "3. **`example_robustness`**:  \n",
    "   Calculates the robustness value, defined as the ratio of the norm of the perturbation to the norm of the input:\n",
    "   $$\n",
    "   \\rho = \\frac{\\|r(x)\\|_2}{\\|x\\|_2}.\n",
    "   $$\n",
    "   This function is compatible with multi-input models, summing norms across all input components.\n",
    "\n",
    "4. **`model_robustness`**:  \n",
    "   Computes the mean and standard deviation of robustness values across multiple examples, providing an overall measure of the model's adversarial robustness.  \n",
    "\n",
    "These functions are designed to evaluate the robustness of multi-input classifiers against adversarial attacks by identifying and quantifying the minimal perturbations required to change predictions. The multi-input support makes this implementation versatile for models that process diverse types of inputs, such as images, text, or combined features. This framework is particularly useful for analyzing and comparing the robustness of different architectures, such as CNNs, RNNs, or hybrid models."
   ],
   "id": "dd7bce0893e6ba42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T18:46:23.998358Z",
     "start_time": "2024-12-01T18:46:23.985360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_gradient(model, x, k):\n",
    "    #Computes the gradient of the k-th element in the model output with respect to the input x.\n",
    "    with tf.GradientTape() as tape:\n",
    "        inputs = {key: tf.cast(value, dtype=tf.float32) for key, value in x.items()}\n",
    "        for value in inputs.values():\n",
    "            tape.watch(value)\n",
    "        results = model(inputs)\n",
    "        # Ensure results is 1D or extract the correct batch element\n",
    "        results_k = results[0, k] if len(results.shape) > 1 else results[k]\n",
    "\n",
    "    gradients = tape.gradient(results_k, inputs)\n",
    "    return {key: grad.numpy() for key, grad in gradients.items()}, results\n",
    "\n",
    "\n",
    "\n",
    "def deepfool(model, x0, eta=0.01, max_iter=20, num_classes=10):\n",
    "    #Implements the DeepFool algorithm for a multi-input model.\n",
    "    # Obtain the initial estimated label\n",
    "    f_x0 = model(x0).numpy().flatten()\n",
    "    label_x0 = np.argmax(f_x0)\n",
    "\n",
    "    loop_i = 0\n",
    "    xi = deepcopy(x0)\n",
    "    label_xi = label_x0\n",
    "\n",
    "    # Main loop\n",
    "    while label_xi == label_x0 and loop_i < max_iter:\n",
    "        w_k_list = []\n",
    "        f_k_list = []\n",
    "        grad_f_label_x0_on_xi, f_xi = get_gradient(model, xi, label_x0)\n",
    "\n",
    "        for k in range(num_classes):\n",
    "            if k == label_x0:\n",
    "                continue\n",
    "            grad_f_k_on_xi, _ = get_gradient(model, xi, k)\n",
    "            w_k = {key: grad_f_k_on_xi[key] - grad_f_label_x0_on_xi[key] for key in xi.keys()}\n",
    "            f_k = f_xi[0, k] - f_xi[0, label_x0]\n",
    "            w_k_norm = np.sqrt(sum(np.linalg.norm(w_k_input.flatten())**2 for w_k_input in w_k.values()))\n",
    "            fk_wk = np.abs(f_k) / (w_k_norm + 1e-8)\n",
    "            w_k_list.append((fk_wk, w_k, f_k))\n",
    "\n",
    "        # Find minimal perturbation\n",
    "        fk_wk_min, w_l, f_l = min(w_k_list, key=lambda t: t[0])\n",
    "\n",
    "        # Compute perturbation\n",
    "        ri_const = np.abs(f_l) / (sum(np.linalg.norm(w_l_input.flatten())**2 for w_l_input in w_l.values()) + 1e-8)\n",
    "        ri = {key: ri_const * w_l_input for key, w_l_input in w_l.items()}\n",
    "\n",
    "        # Update xi\n",
    "        xi = {key: xi[key] + ri[key] for key in xi.keys()}\n",
    "\n",
    "        # Update label\n",
    "        f_xi = model(xi).numpy().flatten()\n",
    "        label_xi = np.argmax(f_xi)\n",
    "        loop_i += 1\n",
    "\n",
    "    # Compute total perturbation\n",
    "    r_sum = {key: (1 + eta) * (xi[key] - x0[key]) for key in x0.keys()}\n",
    "\n",
    "    return r_sum, loop_i, label_xi\n",
    "\n",
    "\n",
    "def example_robustness(x, r, epsilon=1e-8):\n",
    "    #Calculates the robustness value ||r(x)|| / ||x|| for multi-input data.\n",
    "    r_norm = np.sqrt(sum(np.linalg.norm(r_input.flatten())**2 for r_input in r.values()))\n",
    "    x_norm = np.sqrt(sum(np.linalg.norm(x_input.flatten())**2 for x_input in x.values()))\n",
    "    return r_norm / (x_norm + epsilon)\n",
    "\n",
    "\n",
    "\n",
    "def model_robustness(example_robustness_list):\n",
    "    #Calculates the mean and standard deviation of robustness values for the model.\n",
    "    mean = np.mean(np.array(example_robustness_list))\n",
    "    std = np.std(np.array(example_robustness_list))\n",
    "    return mean, std"
   ],
   "id": "98513625e0c6b8cc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Utility functions for saving and loading data with Pickle:",
   "id": "640e7756e6161c2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T18:46:24.185009Z",
     "start_time": "2024-12-01T18:46:24.179090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_pkl(data, path):\n",
    "    with open(path, \"wb\") as saved_data:\n",
    "        pickle.dump(data, saved_data)\n",
    "    saved_data.close()\n",
    "\n",
    "def load_pkl(path):\n",
    "    to_return = None\n",
    "    with open(path, \"rb\") as loaded_data:\n",
    "        to_return = pickle.load(loaded_data)\n",
    "    loaded_data.close()\n",
    "    return to_return"
   ],
   "id": "e1f7afb864e373a9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing and Feature Scaling Functions",
   "id": "dcdcbb005b2e5f9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " This code handles feature preparation for training and evaluation. It includes:\n",
    "- Adjusting feature dimensions to a consistent size using padding or truncation for both 2D (e.g., mel-spectrograms) and 1D features (e.g., spectral rolloff).\n",
    "- Scaling features for normalization, ensuring consistent input distributions.\n",
    "- Splitting data into train, validation, and test sets based on cross-validation folds, organizing the features and labels for each.\n"
   ],
   "id": "85d0a61869e9f524"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T18:46:24.379776Z",
     "start_time": "2024-12-01T18:46:24.372514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to pad or truncate 2D features\n",
    "def pad_or_truncate_2d(X):\n",
    "    num_samples, height, width = X.shape\n",
    "    if width > TARGET_WIDTH:\n",
    "        X = X[:, :, :TARGET_WIDTH]  # Truncate\n",
    "    elif width < TARGET_WIDTH:\n",
    "        pad_width = TARGET_WIDTH - width\n",
    "        X = np.pad(X, ((0, 0), (0, 0), (0, pad_width)), mode='constant')  # Pad\n",
    "    return X\n",
    "\n",
    "# Function to pad or truncate 1D features\n",
    "def pad_or_truncate_1d(X):\n",
    "    num_samples, width = X.shape\n",
    "    if width > TARGET_WIDTH:\n",
    "        X = X[:, :TARGET_WIDTH]  # Truncate\n",
    "    elif width < TARGET_WIDTH:\n",
    "        pad_width = TARGET_WIDTH - width\n",
    "        X = np.pad(X, ((0, 0), (0, pad_width)), mode='constant')  # Pad\n",
    "    return X\n",
    "\n",
    "# Function to scale features\n",
    "def scale_features(X, scaler):\n",
    "    if X.ndim == 3:\n",
    "        num_features = X.shape[1]\n",
    "        X_flat = X.reshape(X.shape[0], -1)\n",
    "        X_scaled_flat = scaler.fit_transform(X_flat)\n",
    "        X_scaled = X_scaled_flat.reshape(X.shape[0], num_features, X.shape[2])\n",
    "    elif X.ndim == 2:\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(f\"Input X must be 2D or 3D array, but got array with shape {X.shape}\")\n",
    "    return X_scaled\n"
   ],
   "id": "ed8a3e9e354db497",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T18:46:24.579105Z",
     "start_time": "2024-12-01T18:46:24.570397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ready_data(fold, features_df, X_mel, X_mfcc, X_chroma, X_contrast,\n",
    "               X_rolloff, X_zcr, y, TARGET_WIDTH=320):\n",
    "    # Use the specified fold as the test set\n",
    "    test_idx = (features_df['fold'] == fold).values\n",
    "\n",
    "    # Use the next fold as the validation set (cycling back to 1 after 10)\n",
    "    validation_fold = (fold % 10) + 1\n",
    "    val_idx = (features_df['fold'] == validation_fold).values\n",
    "\n",
    "    # Use the remaining folds as the training set\n",
    "    train_idx = ~(test_idx | val_idx)\n",
    "\n",
    "\n",
    "    # Adjust features for train, validation, and test sets\n",
    "    X_train_mel = pad_or_truncate_2d(X_mel[train_idx])\n",
    "    X_val_mel = pad_or_truncate_2d(X_mel[val_idx])\n",
    "    X_test_mel = pad_or_truncate_2d(X_mel[test_idx])\n",
    "\n",
    "    X_train_mfcc = pad_or_truncate_2d(X_mfcc[train_idx])\n",
    "    X_val_mfcc = pad_or_truncate_2d(X_mfcc[val_idx])\n",
    "    X_test_mfcc = pad_or_truncate_2d(X_mfcc[test_idx])\n",
    "\n",
    "    X_train_chroma = pad_or_truncate_2d(X_chroma[train_idx])\n",
    "    X_val_chroma = pad_or_truncate_2d(X_chroma[val_idx])\n",
    "    X_test_chroma = pad_or_truncate_2d(X_chroma[test_idx])\n",
    "\n",
    "    X_train_contrast = pad_or_truncate_2d(X_contrast[train_idx])\n",
    "    X_val_contrast = pad_or_truncate_2d(X_contrast[val_idx])\n",
    "    X_test_contrast = pad_or_truncate_2d(X_contrast[test_idx])\n",
    "\n",
    "    X_train_rolloff = pad_or_truncate_1d(X_rolloff[train_idx])\n",
    "    X_val_rolloff = pad_or_truncate_1d(X_rolloff[val_idx])\n",
    "    X_test_rolloff = pad_or_truncate_1d(X_rolloff[test_idx])\n",
    "\n",
    "    X_train_zcr = pad_or_truncate_1d(X_zcr[train_idx])\n",
    "    X_val_zcr = pad_or_truncate_1d(X_zcr[val_idx])\n",
    "    X_test_zcr = pad_or_truncate_1d(X_zcr[test_idx])\n",
    "\n",
    "    # Extract labels\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "    return (X_train_mel, X_train_mfcc, X_train_chroma, X_train_contrast, X_train_rolloff, X_train_zcr, y_train,\n",
    "            X_val_mel, X_val_mfcc, X_val_chroma, X_val_contrast, X_val_rolloff, X_val_zcr, y_val,\n",
    "            X_test_mel, X_test_mfcc, X_test_chroma, X_test_contrast, X_test_rolloff, X_test_zcr, y_test)"
   ],
   "id": "879d20f073cb8ad",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Robustness for All Folds",
   "id": "b48bd9615fcdd771"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we calculate the robustness of the CNN model for each fold that exists in the dataset, we use the same method we did for the CNN training within the 10 folds, and take the same precautions of input sizes and regulating 2D and 1D features.",
   "id": "1c3dc26a879dd5b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:43:29.911778Z",
     "start_time": "2024-12-01T22:43:23.295710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loop over the folds\n",
    "for fold in range(1, 11):\n",
    "    print(f\"Processing fold {fold}...\")\n",
    "\n",
    "    file_path = f\"robustness/robustness_cnn{fold}.pkl\"\n",
    "\n",
    "    # Skip fold if the robustness file already exists\n",
    "    if os.path.exists(file_path):\n",
    "        continue\n",
    "\n",
    "    # Prepare data for this fold using the ready_data function\n",
    "    (X_train_mel, X_train_mfcc, X_train_chroma, X_train_contrast, X_train_rolloff, X_train_zcr, y_train,\n",
    "     X_val_mel, X_val_mfcc, X_val_chroma, X_val_contrast, X_val_rolloff, X_val_zcr, y_val,\n",
    "     X_test_mel, X_test_mfcc, X_test_chroma, X_test_contrast, X_test_rolloff, X_test_zcr, y_test) = ready_data(\n",
    "        fold, features_df, X_mel, X_mfcc, X_chroma, X_contrast, X_rolloff, X_zcr, y\n",
    "    )\n",
    "\n",
    "    # Scale features individually per fold\n",
    "    scaler_mel = StandardScaler()\n",
    "    scaler_mfcc = StandardScaler()\n",
    "    scaler_chroma = StandardScaler()\n",
    "    scaler_contrast = StandardScaler()\n",
    "    scaler_rolloff = StandardScaler()\n",
    "    scaler_zcr = StandardScaler()\n",
    "\n",
    "    # Scale and reshape features\n",
    "    def scale_and_reshape_test(X_train, X_test, scaler):\n",
    "        X_train_scaled = scale_features(X_train, scaler)\n",
    "        X_test_scaled = scaler.transform(X_test.reshape(X_test.shape[0], -1)).reshape(X_test.shape)\n",
    "        return X_train_scaled[..., np.newaxis], X_test_scaled[..., np.newaxis]\n",
    "\n",
    "    X_train_mel_scaled, X_test_mel_scaled = scale_and_reshape_test(X_train_mel, X_test_mel, scaler_mel)\n",
    "    X_train_mfcc_scaled, X_test_mfcc_scaled = scale_and_reshape_test(X_train_mfcc, X_test_mfcc, scaler_mfcc)\n",
    "    X_train_chroma_scaled, X_test_chroma_scaled = scale_and_reshape_test(X_train_chroma, X_test_chroma, scaler_chroma)\n",
    "    X_train_contrast_scaled, X_test_contrast_scaled = scale_and_reshape_test(X_train_contrast, X_test_contrast, scaler_contrast)\n",
    "\n",
    "    # For 1D features (Rolloff and ZCR)\n",
    "    X_train_rolloff_scaled = scale_features(X_train_rolloff, scaler_rolloff)\n",
    "    X_test_rolloff_scaled = scaler_rolloff.transform(X_test_rolloff)\n",
    "    X_train_zcr_scaled = scale_features(X_train_zcr, scaler_zcr)\n",
    "    X_test_zcr_scaled = scaler_zcr.transform(X_test_zcr)\n",
    "\n",
    "    # Combine 1D features after adding channel dimension\n",
    "    def combine_1d_features(rolloff, zcr):\n",
    "        rolloff = rolloff[..., np.newaxis]\n",
    "        zcr = zcr[..., np.newaxis]\n",
    "        return np.concatenate([rolloff, zcr], axis=-1)\n",
    "\n",
    "    X_train_1d = combine_1d_features(X_train_rolloff_scaled, X_train_zcr_scaled)\n",
    "    X_test_1d = combine_1d_features(X_test_rolloff_scaled, X_test_zcr_scaled)\n",
    "\n",
    "    # Load the model for the fold\n",
    "    model_path = f\"assets/kfold_metrics/model_fold{fold}.keras\"\n",
    "    fold_model_cnn = keras.models.load_model(model_path, compile=False)\n",
    "    fold_model_cnn.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    # Initialize the list to hold robustness values\n",
    "    robustness_values_cnn_fold = []\n",
    "    num_test_examples = X_test_mel_scaled.shape[0]\n",
    "\n",
    "        # Run DeepFool for each example\n",
    "    for i in range(num_test_examples):\n",
    "        print(f\"Processing example {i+1}/{num_test_examples} in fold {fold}\")\n",
    "\n",
    "        # Prepare the input for the model\n",
    "        example_input = {\n",
    "            \"mel_input\": np.expand_dims(X_test_mel_scaled[i], axis=0),\n",
    "            \"mfcc_input\": np.expand_dims(X_test_mfcc_scaled[i], axis=0),\n",
    "            \"chroma_input\": np.expand_dims(X_test_chroma_scaled[i], axis=0),\n",
    "            \"contrast_input\": np.expand_dims(X_test_contrast_scaled[i], axis=0),\n",
    "            \"rolloff_input\": np.expand_dims(X_test_1d[i, :, 0:1], axis=0),\n",
    "            \"zcr_input\": np.expand_dims(X_test_1d[i, :, 1:2], axis=0)\n",
    "        }\n",
    "\n",
    "        # Run DeepFool\n",
    "        perturbation, iters, fool_label = deepfool(fold_model_cnn, example_input)\n",
    "\n",
    "        # Compute the robustness value\n",
    "        robustness_value = example_robustness(example_input, perturbation)\n",
    "\n",
    "        robustness_values_cnn_fold.append(robustness_value)\n",
    "\n",
    "        # Save robustness results\n",
    "    os.makedirs(\"robustness\", exist_ok=True)\n",
    "    with open(file_path, \"wb\") as f_out:\n",
    "        pickle.dump(robustness_values_cnn_fold, f_out)\n",
    "    print(f\"Saved robustness results for fold {fold}.\")"
   ],
   "id": "b0c6eab6a2e512fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1...\n",
      "Processing fold 2...\n",
      "Processing fold 3...\n",
      "Processing fold 4...\n",
      "Processing fold 5...\n",
      "Processing fold 6...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 14\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Prepare data for this fold using the ready_data function\u001B[39;00m\n\u001B[0;32m     12\u001B[0m (X_train_mel, X_train_mfcc, X_train_chroma, X_train_contrast, X_train_rolloff, X_train_zcr, y_train,\n\u001B[0;32m     13\u001B[0m  X_val_mel, X_val_mfcc, X_val_chroma, X_val_contrast, X_val_rolloff, X_val_zcr, y_val,\n\u001B[1;32m---> 14\u001B[0m  X_test_mel, X_test_mfcc, X_test_chroma, X_test_contrast, X_test_rolloff, X_test_zcr, y_test) \u001B[38;5;241m=\u001B[39m \u001B[43mready_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeatures_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_mel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_mfcc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_chroma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_contrast\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_rolloff\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_zcr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Scale features individually per fold\u001B[39;00m\n\u001B[0;32m     19\u001B[0m scaler_mel \u001B[38;5;241m=\u001B[39m StandardScaler()\n",
      "Cell \u001B[1;32mIn[6], line 36\u001B[0m, in \u001B[0;36mready_data\u001B[1;34m(fold, features_df, X_mel, X_mfcc, X_chroma, X_contrast, X_rolloff, X_zcr, y, TARGET_WIDTH)\u001B[0m\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m X\n\u001B[0;32m     35\u001B[0m \u001B[38;5;66;03m# Adjust features for train, validation, and test sets\u001B[39;00m\n\u001B[1;32m---> 36\u001B[0m X_train_mel \u001B[38;5;241m=\u001B[39m \u001B[43mpad_or_truncate_2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_mel\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtrain_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m X_val_mel \u001B[38;5;241m=\u001B[39m pad_or_truncate_2d(X_mel[val_idx])\n\u001B[0;32m     38\u001B[0m X_test_mel \u001B[38;5;241m=\u001B[39m pad_or_truncate_2d(X_mel[test_idx])\n",
      "Cell \u001B[1;32mIn[6], line 14\u001B[0m, in \u001B[0;36mready_data.<locals>.pad_or_truncate_2d\u001B[1;34m(X)\u001B[0m\n\u001B[0;32m     11\u001B[0m train_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m~\u001B[39m(test_idx \u001B[38;5;241m|\u001B[39m val_idx)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Function to pad or truncate 2D features\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpad_or_truncate_2d\u001B[39m(X):\n\u001B[0;32m     15\u001B[0m     num_samples, height, width \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m width \u001B[38;5;241m>\u001B[39m TARGET_WIDTH:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Calculate results for each fold",
   "id": "709d866766cbea4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:43:31.845114Z",
     "start_time": "2024-12-01T22:43:31.824820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to load robustness values from a pickle file\n",
    "def load_robustness_values(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Check each fold's models' results\n",
    "for fold in range(1, 11):  # Iterate through folds 1 to 10\n",
    "    file_path = f\"robustness/robustness_cnn{fold}.pkl\"  # Path to the robustness file for each fold\n",
    "\n",
    "    # Ensure the file exists before loading\n",
    "    if os.path.exists(file_path):\n",
    "        robustness_values_cnn_fold = load_robustness_values(file_path)\n",
    "        mean_robustness_cnn, std_robustness_cnn = model_robustness(robustness_values_cnn_fold)\n",
    "        print(f\"Fold {fold} - The CNN model has a robustness of {mean_robustness_cnn:.7f} +/- {std_robustness_cnn:.7f}.\")\n",
    "    else:\n",
    "        print(f\"Fold {fold} - Robustness file not found.\")"
   ],
   "id": "6e66bdfb3ecb4554",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - The CNN model has a robustness of 1.3798980 +/- 4.1650213.\n",
      "Fold 2 - The CNN model has a robustness of 0.8795149 +/- 3.7217876.\n",
      "Fold 3 - The CNN model has a robustness of 1.6633893 +/- 5.6833664.\n",
      "Fold 4 - The CNN model has a robustness of 1.0007375 +/- 3.7238488.\n",
      "Fold 5 - The CNN model has a robustness of 1.3241602 +/- 4.3191468.\n",
      "Fold 6 - Robustness file not found.\n",
      "Fold 7 - Robustness file not found.\n",
      "Fold 8 - Robustness file not found.\n",
      "Fold 9 - Robustness file not found.\n",
      "Fold 10 - Robustness file not found.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The robustness values across folds show high standard deviations compared to their means, indicating the presence of outliers. This suggests that while the model demonstrates moderate robustness on average, certain data points deviate significantly, highlighting cases where the model is either highly robust or extremely vulnerable to adversarial perturbations. These outliers contribute to the variability observed in the robustness measurements, we will now adress these outliers.",
   "id": "5b1d39c2a7f069e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T22:43:39.552166Z",
     "start_time": "2024-12-01T22:43:39.518135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Calculate mean and standard deviation of robustness values\n",
    "mean_robustness = np.mean(robustness_values_cnn_fold)\n",
    "std_robustness = np.std(robustness_values_cnn_fold)\n",
    "\n",
    "# Threshold for outliers\n",
    "threshold_high = mean_robustness + 2 * std_robustness\n",
    "threshold_low = mean_robustness - 2 * std_robustness\n",
    "\n",
    "# Identify outliers\n",
    "outliers = [(i, value) for i, value in enumerate(robustness_values_cnn_fold) if value > threshold_high or value < threshold_low]\n",
    "outlier_indices = [i for i, _ in outliers]\n",
    "\n",
    "print(f\"Number of outliers: {len(outliers)}\")\n",
    "print(\"Outlier indices and values:\", outliers)\n",
    "\n",
    "# Map robustness values to their respective classes\n",
    "outlier_classes = [y_test[idx] for idx in outlier_indices]  # Use y_test instead of test_labels\n",
    "\n",
    "# Count occurrences of each class in outliers\n",
    "outlier_class_counts = Counter(outlier_classes)\n",
    "\n",
    "print(\"Outlier Class Distribution:\")\n",
    "for class_id, count in outlier_class_counts.items():\n",
    "    print(f\"Class {class_id}: {count} occurrences\")\n",
    "\n",
    "# Exclude outliers from robustness values\n",
    "filtered_robustness_values = [value for i, value in enumerate(robustness_values_cnn_fold) if i not in outlier_indices]\n",
    "\n",
    "# Recalculate mean and standard deviation without outliers\n",
    "filtered_mean_robustness = np.mean(filtered_robustness_values)\n",
    "filtered_std_robustness = np.std(filtered_robustness_values)\n",
    "\n",
    "print(f\"Filtered Mean Robustness: {filtered_mean_robustness}\")\n",
    "print(f\"Filtered Standard Deviation Robustness: {filtered_std_robustness}\")\n"
   ],
   "id": "c11a393edd1a8d0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 44\n",
      "Outlier indices and values: [(134, np.float64(12.784296952876474)), (136, np.float64(24.144619940274744)), (137, np.float64(11.57103600590289)), (138, np.float64(18.514620831131403)), (139, np.float64(17.47469266491729)), (140, np.float64(15.067606740929234)), (141, np.float64(17.216584507119425)), (143, np.float64(13.984420965162009)), (147, np.float64(20.728384694158294)), (149, np.float64(15.314793961093748)), (150, np.float64(15.908570787150246)), (151, np.float64(26.652310187457466)), (152, np.float64(23.353275061638115)), (155, np.float64(15.294607511530602)), (201, np.float64(14.363248989424012)), (262, np.float64(13.21630838101174)), (268, np.float64(14.94779430493747)), (437, np.float64(11.047122847140068)), (544, np.float64(14.907048086684412)), (581, np.float64(14.145543287390014)), (592, np.float64(26.016408985380888)), (593, np.float64(29.477554521607978)), (596, np.float64(28.696355460876095)), (599, np.float64(18.126364999018605)), (600, np.float64(24.41981565053108)), (601, np.float64(17.922324363822906)), (602, np.float64(28.60904417503712)), (603, np.float64(31.88898495454084)), (604, np.float64(14.93876025628201)), (605, np.float64(10.811120840755645)), (711, np.float64(21.4767828540268)), (712, np.float64(14.757370105790391)), (713, np.float64(11.512838087875497)), (797, np.float64(24.123345739443998)), (802, np.float64(25.063337746796893)), (806, np.float64(16.357767570581974)), (818, np.float64(31.440277312348346)), (819, np.float64(23.065394733932678)), (823, np.float64(14.037535190497227)), (836, np.float64(22.382744127054362)), (837, np.float64(24.862057470673722)), (838, np.float64(17.20632375754317)), (853, np.float64(16.891468533061943)), (909, np.float64(15.318216294273459))]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 823 is out of bounds for axis 0 with size 823",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 14\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOutlier indices and values:\u001B[39m\u001B[38;5;124m\"\u001B[39m, outliers)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Map robustness values to their respective classes\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m outlier_classes \u001B[38;5;241m=\u001B[39m [\u001B[43my_test\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx, _ \u001B[38;5;129;01min\u001B[39;00m outliers]  \u001B[38;5;66;03m# Use y_test instead of test_labels\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Count occurrences of each class in outliers\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Counter\n",
      "\u001B[1;31mIndexError\u001B[0m: index 823 is out of bounds for axis 0 with size 823"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "88afb1139ae59073"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RNN Model",
   "id": "1a5a0e86de614acb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preparation for the RNN",
   "id": "ff7f3879f0a9d63f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load preprocessed features (e.g., Mel Spectrograms)\n",
    "with open('rnn_features.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Extract feature arrays and labels\n",
    "features_df = pd.DataFrame(data)\n",
    "X_mel = np.array(features_df['mel_spec'].tolist())  # Use 'mel_spec' key for Mel Spectrograms\n",
    "X_class = np.array(features_df['classID'].tolist())\n",
    "folds = np.array(features_df['fold'])\n",
    "\n",
    "# Reshape Mel spectrograms for RNN input (samples, time_steps, freq_bins)\n",
    "X_mel = X_mel.reshape(X_mel.shape[0], X_mel.shape[1], X_mel.shape[2])  # Remove the channel dimension\n",
    "print(f\"Shape of Mel spectrogram input: {X_mel.shape}\")"
   ],
   "id": "55a5b9429645197"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Iterate over 10 folds",
   "id": "87ec2625aea0f3b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Loop over 10 folds\n",
    "for fold in range(1, 11):\n",
    "    print(f\"Processing fold {fold}...\")\n",
    "\n",
    "    # File to save robustness results\n",
    "    file_path = f\"robustness/rnn_robustness_fold{fold}.pkl\"\n",
    "\n",
    "    # Skip fold if results already exist\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Skipping fold {fold}, results already exist.\")\n",
    "        continue\n",
    "\n",
    "    # Train-test-validation split\n",
    "    test_idx = (folds == fold)\n",
    "    train_idx = ~test_idx\n",
    "\n",
    "    X_train, X_test = X_mel[train_idx], X_mel[test_idx]\n",
    "    y_train, y_test = X_class[train_idx], X_class[test_idx]\n",
    "\n",
    "    # Load the RNN model for the current fold\n",
    "    model_path = f\"rnn_models/rnn_model_fold{fold}.keras\"\n",
    "    rnn_model = keras.models.load_model(model_path, compile=False)\n",
    "    rnn_model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    # Initialize list for robustness values\n",
    "    robustness_values_rnn_fold = []\n",
    "    num_test_examples = X_test.shape[0]\n",
    "\n",
    "    # Run DeepFool on each test example\n",
    "    for i in range(num_test_examples):\n",
    "        print(f\"Processing example {i + 1}/{num_test_examples} in fold {fold}...\")\n",
    "\n",
    "        # Prepare input (ensure the shape matches RNN's expected input)\n",
    "        example_input = np.expand_dims(X_test[i], axis=0)  # Shape: (1, time_steps, freq_bins)\n",
    "\n",
    "        # Apply DeepFool\n",
    "        perturbation, iters, fool_label = deepfool(rnn_model, example_input)\n",
    "\n",
    "        # Compute robustness value\n",
    "        robustness_value = example_robustness(example_input, perturbation)\n",
    "\n",
    "        # Store robustness value\n",
    "        robustness_values_rnn_fold.append(robustness_value)\n",
    "\n",
    "    # Save robustness results\n",
    "    os.makedirs(\"robustness\", exist_ok=True)\n",
    "    with open(file_path, \"wb\") as f_out:\n",
    "        pickle.dump(robustness_values_rnn_fold, f_out)\n",
    "    print(f\"Saved robustness results for fold {fold}.\")"
   ],
   "id": "52386428704d11e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analyze results",
   "id": "b6bf6091ddc5a82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check each fold's models' results\n",
    "for fold in range(1, 11):  # Iterate through folds 1 to 10\n",
    "    file_path = f\"robustness/robustness_rnn{fold}.pkl\"  # Path to the robustness file for each fold\n",
    "\n",
    "    # Ensure the file exists before loading\n",
    "    if os.path.exists(file_path):\n",
    "        robustness_values_rnn_fold = load_robustness_values(file_path)\n",
    "        mean_robustness_rnn, std_robustness_rnn = model_robustness(robustness_values_rnn_fold)\n",
    "        print(f\"Fold {fold} - The RNN model has a robustness of {mean_robustness_rnn:.7f} +/- {std_robustness_rnn:.7f}.\")\n",
    "    else:\n",
    "        print(f\"Fold {fold} - Robustness file not found.\")"
   ],
   "id": "d99c066247e8e9e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate mean and standard deviation of robustness values\n",
    "mean_robustness = np.mean(robustness_values_rnn_fold)\n",
    "std_robustness = np.std(robustness_values_rnn_fold)\n",
    "\n",
    "# Define thresholds for outliers (Â±2 standard deviations)\n",
    "threshold_high = mean_robustness + 2 * std_robustness\n",
    "threshold_low = mean_robustness - 2 * std_robustness\n",
    "\n",
    "# Identify outliers (indices and values)\n",
    "outliers = [(i, value) for i, value in enumerate(robustness_values_rnn_fold)\n",
    "            if value > threshold_high or value < threshold_low]\n",
    "\n",
    "# Print summary of outliers\n",
    "print(f\"Number of outliers: {len(outliers)}\")\n",
    "print(\"Outlier indices and values:\", outliers)\n",
    "\n",
    "# Ensure y_test contains class IDs, not one-hot encoding\n",
    "if len(y_test.shape) > 1 and y_test.shape[1] > 1:\n",
    "    y_test = np.argmax(y_test, axis=1)  # Convert one-hot to class IDs\n",
    "\n",
    "# Map robustness values to their respective classes\n",
    "outlier_classes = [y_test[idx] for idx, _ in outliers]\n",
    "\n",
    "# Count occurrences of each class in outliers\n",
    "from collections import Counter\n",
    "outlier_class_counts = Counter(outlier_classes)\n",
    "\n",
    "# Print outlier class distribution\n",
    "print(\"Outlier Class Distribution:\")\n",
    "for class_id, count in outlier_class_counts.items():\n",
    "    print(f\"Class {class_id}: {count} occurrences\")\n"
   ],
   "id": "28b95b7243701e94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Conclusion",
   "id": "1b029da164ec2481"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3adf3459677c4e3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# References",
   "id": "dab5230da37f78b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "   - [Moosavi-Dezfooli, S.-M., Fawzi, A., Frossard, P., Polytechnique, E. and De Lausanne, F. (2016). DeepFool: a simple and accurate method to fool deep neural networks.](https://openaccess.thecvf.com/content_cvpr_2016/papers/Moosavi-Dezfooli_DeepFool_A_Simple_CVPR_2016_paper.pdf)\n",
    "\n",
    "   - [Morgan, A. (2022). A Review of DeepFool: a simple and accurate method to fool deep neural networks. [online] Machine Intelligence and Deep Learning](https://medium.com/machine-intelligence-and-deep-learning-lab/a-review-of-deepfool-a-simple-and-accurate-method-to-fool-deep-neural-networks-b016fba9e48e#:~:text=DeepFool%20finds%20the%20minimal%20perturbations)\n",
    "\n"
   ],
   "id": "8376e3a16523019d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c4bc10ad2246e896"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
